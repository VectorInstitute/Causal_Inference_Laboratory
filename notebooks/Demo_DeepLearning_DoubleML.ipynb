{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvETxBng4Sc2jFmqoGkQ+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VectorInstitute/Causal_Inference_Laboratory/blob/main/notebooks/Demo_DeepLearning_DoubleML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Inference via Deep Learning and Double Machine Learning\n",
        "\n",
        "In this tutorial, we will demo causal inference (CI) based on deep learning (DL) models and other models tailored for CI, including\n",
        "- Treatment-Agnostic Representation Network (TAR-Net) [1]\n",
        "- Dragon-Net [2]\n",
        "- Double Machine Learning [3]\n",
        "\n",
        "\n",
        "\n",
        "[1] U. Shalit, F. D. Johansson, and D. Sontag, “Estimating individual treatment effect: generalization bounds and algorithms,” in ICML 2017.\n",
        "\n",
        "[2] C. Shi, D. Blei, and V. Veitch, “Adapting neural networks for the estimation of treatment effects,” in NeurIPS 2019.\n",
        "\n",
        "[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins, ”Double/debiased machine learning for treatment and structural parameters,” The Econometrics Journal, vol. 21, no. 1, pp. 1–68, Jan 2018."
      ],
      "metadata": {
        "id": "jlTN1rkvTnAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "We will use some modules from our public repo on https://github.com/VectorInstitute/Causal_Inference_Laboratory.git"
      ],
      "metadata": {
        "id": "qg0MznhgZTm1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vd7mfQP0Tjwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69fb690-b4dc-40df-f49b-b2ab96eac071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Causal_Inference_Laboratory'...\n",
            "remote: Enumerating objects: 489, done.\u001b[K\n",
            "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 489 (delta 90), reused 84 (delta 40), pack-reused 333\u001b[K\n",
            "Receiving objects: 100% (489/489), 27.99 MiB | 23.71 MiB/s, done.\n",
            "Resolving deltas: 100% (229/229), done.\n",
            "mv: cannot move 'Causal_Inference_Laboratory' to 'code/Causal_Inference_Laboratory': Directory not empty\n",
            "mv: cannot stat 'code/data': No such file or directory\n",
            "mv: cannot stat 'code/utils': No such file or directory\n",
            "mv: cannot stat 'code/models': No such file or directory\n",
            "mv: cannot stat 'code/estimation_results': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/VectorInstitute/Causal_Inference_Laboratory.git\n",
        "!mv Causal_Inference_Laboratory code\n",
        "!mv code/data ./data\n",
        "!mv code/utils ./utils\n",
        "!mv code/models ./models\n",
        "!mv code/estimation_results ./estimation_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from typing import Dict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "\n",
        "\n",
        "import utils.preprocessing as helper\n",
        "from utils.preprocessing import sys_config"
      ],
      "metadata": {
        "id": "BK3yrSPtlATP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "In this demo, we will use the Infant Health and Development Program (IHDP) dataset [4].\n",
        "\n",
        "- from a randomized experiment studying the effect of home visits by specialists on future cognitive test scores of children;\n",
        "- the children of non-white mothers in the treated set are removed to de-randomize the experiment;\n",
        "- each unit is simulated for a treated and a control outcome;\n",
        "\n",
        "Details:\n",
        "- the train split (672 units for each realization) and the test split (75 units for each realization) \n",
        "- x: covariates;\n",
        "- t: treatment;\n",
        "- yf: factual outcome;\n",
        "- ycf: counterfactual outcome;\n",
        "- mu0: noiseless potential control outcome;\n",
        "- mu1: noiseless potential treated outcome.\n",
        "\n",
        "\n",
        "\n",
        "*IHDP-100/1000 contains 100/1000 realizations. In our demo, we will just use the first 5 realizations of IHDP-100.*\n",
        "\n",
        "[4] J. L. Hill, “Bayesian nonparametric modeling for causal inference,” Journal of Computational and Graphical Statistics, vol. 20, no. 1, pp. 217–240, 2011. [Online]. Available: https://doi.org/10.1198/jcgs.2010.08162"
      ],
      "metadata": {
        "id": "GyKqnASQg7RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_folder = sys_config[\"datasets_folder\"]\n",
        "results_folder = sys_config[\"results_folder\"]\n",
        "\n",
        "dataset_name = \"IHDP-100\" \n",
        "num_realizations = 5\n",
        "\n",
        "x_train_all, t_train_all, yf_train_all = helper.load_IHDP_observational(\n",
        "        datasets_folder, dataset_name, details=True\n",
        "    )\n",
        "x_test_all, t_test_all, yf_test_all = helper.load_IHDP_out_of_sample(\n",
        "    datasets_folder, dataset_name, details=True\n",
        ")\n",
        "\n",
        "x_train, t_train, yf_train = x_train_all[:, :, :num_realizations], t_train_all[:, :num_realizations], yf_train_all[:, :num_realizations]\n",
        "x_test = x_test_all[:, :, :num_realizations]\n",
        "print(x_train[0, :, 0])"
      ],
      "metadata": {
        "id": "_JR44Vjpg47Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3b1f53-5e03-41f6-a8d0-a8ce0d218429"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "The details of the train split of IHDP-100 dataset:\n",
            "Number of realizations: 100\n",
            "ate () int64\n",
            "mu1 (672, 100) float64\n",
            "mu0 (672, 100) float64\n",
            "yadd () int64\n",
            "yf (672, 100) float64\n",
            "ycf (672, 100) float64\n",
            "t (672, 100) float64\n",
            "x (672, 25, 100) float64\n",
            "ymul () int64\n",
            "-------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------\n",
            "The details of the test split of IHDP-100 dataset:\n",
            "Number of realizations: 100\n",
            "ate () int64\n",
            "mu1 (75, 100) float64\n",
            "mu0 (75, 100) float64\n",
            "yadd () int64\n",
            "yf (75, 100) float64\n",
            "ycf (75, 100) float64\n",
            "t (75, 100) float64\n",
            "x (75, 25, 100) float64\n",
            "ymul () int64\n",
            "-------------------------------------------------------------------------------\n",
            "[ 0.31658816  0.59658219 -0.36089799 -0.87960599  0.37108604 -1.18901798\n",
            "  1.          0.          1.          0.          1.          0.\n",
            "  0.          2.          0.          1.          0.          1.\n",
            "  0.          1.          0.          0.          0.          0.\n",
            "  0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "A1btUwnRyf8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define neural network architectures\n",
        "class EpsilonLayer(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(EpsilonLayer, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.epsilon = self.add_weight(\n",
        "            name=\"epsilon\",\n",
        "            shape=[1, 1],\n",
        "            initializer=\"RandomNormal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        super(EpsilonLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
        "\n",
        "\n",
        "def make_dragonnet(input_dim, reg_l2):\n",
        "    \"\"\"\n",
        "    Create the dragonnet which has three heads.\n",
        "    :param input_dim:\n",
        "    :param reg_l2:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(input_dim,), name=\"input\")\n",
        "\n",
        "    # representation\n",
        "    x = layers.Dense(\n",
        "        units=200, activation=\"elu\", kernel_initializer=\"RandomNormal\"\n",
        "    )(inputs)\n",
        "    x = layers.Dense(\n",
        "        units=200, activation=\"elu\", kernel_initializer=\"RandomNormal\"\n",
        "    )(x)\n",
        "    x = layers.Dense(\n",
        "        units=200, activation=\"elu\", kernel_initializer=\"RandomNormal\"\n",
        "    )(x)\n",
        "\n",
        "    t_predictions = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # two heads for two hypotheses\n",
        "    y0_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(x)\n",
        "    y1_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(x)\n",
        "\n",
        "    # second layer\n",
        "    y0_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(y0_hidden)\n",
        "    y1_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(y1_hidden)\n",
        "\n",
        "    # third layer\n",
        "    y0_predictions = layers.Dense(\n",
        "        units=1, kernel_regularizer=regularizers.L2(reg_l2), name=\"y0_pred\"\n",
        "    )(y0_hidden)\n",
        "    y1_predictions = layers.Dense(\n",
        "        units=1, kernel_regularizer=regularizers.L2(reg_l2), name=\"y1_pred\"\n",
        "    )(y1_hidden)\n",
        "\n",
        "    dl = EpsilonLayer()\n",
        "    epsilons = dl(t_predictions, name=\"epsilon\")\n",
        "    concat_pred = layers.Concatenate(axis=1)(\n",
        "        [y0_predictions, y1_predictions, t_predictions, epsilons]\n",
        "    )\n",
        "    model = keras.Model(inputs=inputs, outputs=concat_pred, name=\"dragonnet\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_tarnet(input_dim, reg_l2):\n",
        "    \"\"\"\n",
        "    Create the TARNet which has three heads.\n",
        "    :param input_dim:\n",
        "    :param reg_l2:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(input_dim,), name=\"input\")\n",
        "\n",
        "    # representation\n",
        "    x = layers.Dense(\n",
        "        units=200, activation=\"elu\", kernel_initializer=\"RandomNormal\"\n",
        "    )(inputs)\n",
        "    x = layers.Dense(\n",
        "        units=200, activation=\"elu\", kernel_initializer=\"RandomNormal\"\n",
        "    )(x)\n",
        "    x = layers.Dense(\n",
        "        units=200, activation=\"elu\", kernel_initializer=\"RandomNormal\"\n",
        "    )(x)\n",
        "\n",
        "    # different from Dragonnet, here TARNet directly uses inputs to predict t\n",
        "    t_predictions = layers.Dense(units=1, activation=\"sigmoid\")(inputs)\n",
        "\n",
        "    # two heads for two hypotheses\n",
        "    y0_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(x)\n",
        "    y1_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(x)\n",
        "\n",
        "    # second layer\n",
        "    y0_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(y0_hidden)\n",
        "    y1_hidden = layers.Dense(\n",
        "        units=100, activation=\"elu\", kernel_regularizer=regularizers.L2(reg_l2)\n",
        "    )(y1_hidden)\n",
        "\n",
        "    # third layer\n",
        "    y0_predictions = layers.Dense(\n",
        "        units=1, kernel_regularizer=regularizers.L2(reg_l2), name=\"y0_pred\"\n",
        "    )(y0_hidden)\n",
        "    y1_predictions = layers.Dense(\n",
        "        units=1, kernel_regularizer=regularizers.L2(reg_l2), name=\"y1_pred\"\n",
        "    )(y1_hidden)\n",
        "\n",
        "    dl = EpsilonLayer()\n",
        "    epsilons = dl(t_predictions, name=\"epsilon\")\n",
        "    concat_pred = layers.Concatenate(axis=1)(\n",
        "        [y0_predictions, y1_predictions, t_predictions, epsilons]\n",
        "    )\n",
        "    model = keras.Model(inputs=inputs, outputs=concat_pred, name=\"tarnet\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# define customized loss functions\n",
        "def regression_loss(concat_true, concat_pred):\n",
        "    # concat_true: [y, t]\n",
        "    # concat_pred: [y0_predictions, y1_predictions, t_predictions, epsilons]\n",
        "    y_true = concat_true[:, 0]\n",
        "    t_true = concat_true[:, 1]\n",
        "\n",
        "    y0_pred = concat_pred[:, 0]\n",
        "    y1_pred = concat_pred[:, 1]\n",
        "\n",
        "    # if treatment is 0, compare the factual y (y0) and y0_pred\n",
        "    loss0 = tf.reduce_sum((1.0 - t_true) * tf.square(y_true - y0_pred))\n",
        "\n",
        "    # if treatment is 1, compare the factual y (y1) and y1_pred\n",
        "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
        "    return loss0 + loss1\n",
        "\n",
        "def binary_classification_loss(concat_true, concat_pred):\n",
        "    t_true = concat_true[:, 1]\n",
        "    t_pred = concat_pred[:, 2]\n",
        "    t_pred = (t_pred + 0.001) / 1.002\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    losst = tf.reduce_sum(bce(t_true, t_pred))\n",
        "    return losst\n",
        "\n",
        "def dragonnet_loss_binarycross(concat_true, concat_pred):\n",
        "    return regression_loss(\n",
        "        concat_true, concat_pred\n",
        "    ) + binary_classification_loss(concat_true, concat_pred)\n",
        "\n",
        "def make_tarreg_loss(ratio=1.0, dragonnet_loss=dragonnet_loss_binarycross):\n",
        "    def tarreg_ATE_unbounded_domain_loss(concat_true, concat_pred):\n",
        "        vanilla_loss = dragonnet_loss(concat_true, concat_pred)\n",
        "\n",
        "        y_true = concat_true[:, 0]\n",
        "        t_true = concat_true[:, 1]\n",
        "\n",
        "        y0_pred = concat_pred[:, 0]\n",
        "        y1_pred = concat_pred[:, 1]\n",
        "        t_pred = concat_pred[:, 2]\n",
        "\n",
        "        epsilons = concat_pred[:, 3]\n",
        "        t_pred = (t_pred + 0.01) / 1.02\n",
        "\n",
        "        y_pred = t_true * y1_pred + (1 - t_true) * y0_pred\n",
        "\n",
        "        h = t_true / t_pred - (1 - t_true) / (1 - t_pred)\n",
        "\n",
        "        y_pert = y_pred + epsilons * h\n",
        "        targeted_regularization = tf.reduce_sum(tf.square(y_true - y_pert))\n",
        "\n",
        "        loss = vanilla_loss + ratio * targeted_regularization\n",
        "        return loss\n",
        "\n",
        "    return tarreg_ATE_unbounded_domain_loss\n",
        "    \n",
        "# define other metrics\n",
        "def treatment_accuracy(concat_true, concat_pred):\n",
        "    t_true = concat_true[:, 1]\n",
        "    t_pred = concat_pred[:, 2]\n",
        "    return tf.keras.metrics.binary_accuracy(t_true, t_pred)\n",
        "\n",
        "def track_epsilon(concat_true, concat_pred):\n",
        "    epsilons = concat_pred[:, 3]\n",
        "    return tf.abs(tf.reduce_mean(epsilons))"
      ],
      "metadata": {
        "id": "GMLu9uX4C63g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation via TARNet and Dragonnet"
      ],
      "metadata": {
        "id": "SYc4ogvYylXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_estimate_Dragonnet(x, t, y_unscaled, x_test, estimator_name):\n",
        "    \"\"\"\n",
        "    Training using Drogonnet.\n",
        "    :param x: covariates\n",
        "    :param t: treatment\n",
        "    :param yf: factual outcomes\n",
        "    :param x_test: out-of-sample covariates\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    reg_l2 = 0.01\n",
        "    batch_size = 32\n",
        "\n",
        "    # 6 cont. features + 19 discrete features => 19 discrete + 6 cont. features\n",
        "    num_features = x.shape[1]\n",
        "    if num_features == 25:  # IHDP\n",
        "        perm = np.arange(6, 25).tolist() + np.arange(6).tolist()\n",
        "        x = x[:, perm]\n",
        "        x_test = x_test[:, perm]\n",
        "    t = t[:, None]  # add a dummy axis (from original dragonnet code)\n",
        "    y_unscaled = y_unscaled[:, None]\n",
        "\n",
        "    # y_scaler = StandardScaler().fit(y_unscaled)  # from dragonnet code\n",
        "    # y = y_scaler.transform(y_unscaled) # normalize y from original code\n",
        "    y = y_unscaled\n",
        "    if estimator_name == \"Dragonnet\":\n",
        "        model = make_dragonnet(x.shape[-1], reg_l2)\n",
        "    elif estimator_name == \"TARNet\":\n",
        "        model = make_tarnet(x.shape[-1], reg_l2)\n",
        "\n",
        "    metrics = [\n",
        "        regression_loss,\n",
        "        binary_classification_loss,\n",
        "        treatment_accuracy,\n",
        "        track_epsilon,\n",
        "    ]\n",
        "\n",
        "    loss = make_tarreg_loss(\n",
        "        ratio=1.0, dragonnet_loss=dragonnet_loss_binarycross\n",
        "    )\n",
        "\n",
        "    yt_train = np.concatenate([y, t], axis=1)\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    # from original code, training in two phasesL use a high learning rate first, \n",
        "    # then use a low learning rate\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "    )\n",
        "    \n",
        "    verbose = 0\n",
        "    adam_callbacks = [\n",
        "        keras.callbacks.TerminateOnNaN(),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\", patience=2, min_delta=0.0\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"loss\",\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=verbose,\n",
        "            mode=\"auto\",\n",
        "            min_delta=1e-8,\n",
        "            cooldown=0,\n",
        "            min_lr=0,\n",
        "        ),\n",
        "    ]\n",
        "    \n",
        "    model.fit(\n",
        "        x,\n",
        "        yt_train,\n",
        "        callbacks=adam_callbacks,\n",
        "        validation_split=0.2,\n",
        "        epochs=100,\n",
        "        batch_size=batch_size,\n",
        "        verbose=verbose,\n",
        "    )\n",
        "\n",
        "\n",
        "    sgd_callbacks = [\n",
        "        keras.callbacks.TerminateOnNaN(),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\", patience=40, min_delta=0.0\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"loss\",\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=verbose,\n",
        "            mode=\"auto\",\n",
        "            min_delta=0.0,\n",
        "            cooldown=0,\n",
        "            min_lr=0,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.SGD(\n",
        "            learning_rate=1e-5, momentum=0.9, nesterov=True\n",
        "        ),\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x,\n",
        "        yt_train,\n",
        "        callbacks=sgd_callbacks,\n",
        "        validation_split=0.2,\n",
        "        epochs=300,\n",
        "        batch_size=batch_size,\n",
        "        verbose=verbose,\n",
        "    )\n",
        "\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    text = f\"Elapsed_time is: {elapsed_time:.4f} seconds\"\n",
        "    print(f\"{text:-^79}\")\n",
        "\n",
        "    yt_hat_test = model.predict(x_test)\n",
        "    yt_hat_train = model.predict(x)\n",
        "    # y0_in, y1_in = _split_output(yt_hat_train, t, y_scaler, 'in')\n",
        "    # y0_out, y1_out = _split_output(yt_hat_test, None, y_scaler, 'out')\n",
        "    y0_in, y1_in = yt_hat_train[:, 0], yt_hat_train[:, 1]\n",
        "    ate_in = np.mean(y1_in - y0_in)\n",
        "    y0_out, y1_out = yt_hat_test[:, 0], yt_hat_test[:, 1]\n",
        "    ate_out = np.mean(y1_out - y0_out)\n",
        "\n",
        "    # propensity_in = yt_hat_train[:, 2]\n",
        "    # propensity_out = yt_hat_test[:, 2]\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    # return y0_in, y1_in, ate_in, y0_out, y1_out, ate_out, propensity_in, propensity_out\n",
        "    return y0_in, y1_in, ate_in, y0_out, y1_out, ate_out\n",
        "\n"
      ],
      "metadata": {
        "id": "MQdxT-ZI_AIt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimator_set = [\"TARNet\", \"Dragonnet\"]\n",
        "for estimator_name in estimator_set:\n",
        "        y0_in_all, y1_in_all, y0_out_all, y1_out_all = [], [], [], []\n",
        "        ate_in_all, ate_out_all = [], []\n",
        "        for i in range(num_realizations):\n",
        "            text = f\" Estimation of realization {i} via {estimator_name}\"\n",
        "            print(f\"{text:-^79}\")\n",
        "            x, t, yf = x_train[:, :, i], t_train[:, i], yf_train[:, i]\n",
        "            x_t = x_test[:, :, i]\n",
        "            # train the estimator and predict for this realization\n",
        "            (\n",
        "                y0_in,\n",
        "                y1_in,\n",
        "                ate_in,\n",
        "                y0_out,\n",
        "                y1_out,\n",
        "                ate_out,\n",
        "            ) = train_and_estimate_Dragonnet(x, t, yf, x_t, estimator_name)\n",
        "            y0_in_all.append(y0_in)\n",
        "            y1_in_all.append(y1_in)\n",
        "            ate_in_all.append(ate_in)\n",
        "            y0_out_all.append(y0_out)\n",
        "            y1_out_all.append(y1_out)\n",
        "            ate_out_all.append(ate_out)\n",
        "        # follow the dimension order of the dataset,\n",
        "        # i.e., realizations are captured by the last index\n",
        "        y0_in_all = np.squeeze(np.array(y0_in_all).transpose())\n",
        "        y1_in_all = np.squeeze(np.array(y1_in_all).transpose())\n",
        "        y0_out_all = np.squeeze(np.array(y0_out_all).transpose())\n",
        "        y1_out_all = np.squeeze(np.array(y1_out_all).transpose())\n",
        "        ate_in_all = np.array(ate_in_all)\n",
        "        ate_out_all = np.array(ate_out_all)\n",
        "\n",
        "        # save estimation results\n",
        "        estimation_result_folder = os.path.join(\n",
        "            results_folder, dataset_name, estimator_name\n",
        "        )\n",
        "        print(f\"Saving {estimation_result_folder}.\")\n",
        "        helper.save_in_and_out_results(\n",
        "            estimation_result_folder,\n",
        "            y0_in_all,\n",
        "            y1_in_all,\n",
        "            ate_in_all,\n",
        "            y0_out_all,\n",
        "            y1_out_all,\n",
        "            ate_out_all,\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUQ1b4tQNwTU",
        "outputId": "1518569b-0c3d-4ecd-ce3c-d9d030da4f91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Estimation of realization 0 via TARNet--------------------\n",
            "-----------------------Elapsed_time is: 19.2062 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "-------------------- Estimation of realization 1 via TARNet--------------------\n",
            "-----------------------Elapsed_time is: 26.2874 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "-------------------- Estimation of realization 2 via TARNet--------------------\n",
            "-----------------------Elapsed_time is: 20.0839 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "-------------------- Estimation of realization 3 via TARNet--------------------\n",
            "-----------------------Elapsed_time is: 25.1130 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "-------------------- Estimation of realization 4 via TARNet--------------------\n",
            "-----------------------Elapsed_time is: 17.3324 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "Saving ./estimation_results/IHDP-100/TARNet.\n",
            "------------------ Estimation of realization 0 via Dragonnet-------------------\n",
            "-----------------------Elapsed_time is: 16.3977 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "------------------ Estimation of realization 1 via Dragonnet-------------------\n",
            "-----------------------Elapsed_time is: 21.5666 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "------------------ Estimation of realization 2 via Dragonnet-------------------\n",
            "-----------------------Elapsed_time is: 17.7407 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "------------------ Estimation of realization 3 via Dragonnet-------------------\n",
            "-----------------------Elapsed_time is: 20.5738 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "------------------ Estimation of realization 4 via Dragonnet-------------------\n",
            "-----------------------Elapsed_time is: 20.3206 seconds------------------------\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "Saving ./estimation_results/IHDP-100/Dragonnet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation via Double Machine Learning"
      ],
      "metadata": {
        "id": "Ia4Wgflyyrj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
        "\n",
        "def train_and_estimate_DML_helper(x_aux, x_new, t_aux, t_new, yf_aux, yf_new):\n",
        "    # use auxiliary data to fit t from x and y from x\n",
        "    LR = LogisticRegression()\n",
        "    LR.fit(x_aux, t_aux)\n",
        "\n",
        "    LS = Ridge()\n",
        "    LS.fit(x_aux, yf_aux)\n",
        "\n",
        "    # get residuals \n",
        "    t_r = t_new - LR.predict_proba(x_new)[:, 1]\n",
        "    y_r = yf_new - LS.predict(x_new)\n",
        "\n",
        "    OLS = LinearRegression()\n",
        "    OLS.fit(t_r.reshape(-1, 1), y_r)  # single feature\n",
        "\n",
        "    ate_in, ate_out = OLS.coef_.item(), OLS.coef_.item()\n",
        "    return ate_in, ate_out\n",
        "\n",
        "def train_and_estimate_DML(x, t, yf, x_test):\n",
        "    \"\"\"\n",
        "    Training using double machine learning (DML), i.e., R-Learner.\n",
        "    :param x: covariates\n",
        "    :param t: treatment\n",
        "    :param yf: factual outcomes\n",
        "    :param x_test: out-of-sample covariates\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    x_aux, x_new, t_aux, t_new, yf_aux, yf_new = train_test_split(\n",
        "        x, t, yf, test_size=0.5, shuffle=False\n",
        "    )\n",
        "\n",
        "    ate_in_1, ate_out_1 = train_and_estimate_DML_helper(x_aux, x_new, t_aux, t_new, yf_aux, yf_new)\n",
        "    ate_in_2, ate_out_2 = train_and_estimate_DML_helper(x_new, x_aux, t_new, t_aux, yf_new, yf_aux)\n",
        "\n",
        "    return (ate_in_1+ate_in_2)/2.0, (ate_out_1+ate_out_2)/2.0\n",
        "\n",
        "\n",
        "estimator_set = [\"DML\"]\n",
        "for estimator_name in estimator_set:\n",
        "    ate_in_all = []\n",
        "    ate_out_all = []\n",
        "    for i in range(num_realizations):\n",
        "        text = f\" Estimation of realization {i} via {estimator_name}\"\n",
        "        print(f\"{text:-^79}\")\n",
        "        x, t, yf = x_train[:, :, i], t_train[:, i], yf_train[:, i]\n",
        "        x_t = x_test[:, :, i]\n",
        "        ate_in, ate_out = train_and_estimate_DML(x, t, yf, x_t)\n",
        "        ate_in_all.append(ate_in)\n",
        "        ate_out_all.append(ate_out)\n",
        "\n",
        "    y0_in_all = None\n",
        "    y1_in_all = None\n",
        "    y0_out_all = None\n",
        "    y1_out_all = None\n",
        "    ate_in_all = np.array(ate_in_all)\n",
        "    ate_out_all = np.array(ate_in_all)\n",
        "    # save estimation results\n",
        "    estimation_result_folder = os.path.join(\n",
        "        results_folder, dataset_name, estimator_name\n",
        "    )\n",
        "    print(f\"Saving {estimation_result_folder}.\")\n",
        "    helper.save_in_and_out_results(\n",
        "        estimation_result_folder,\n",
        "        y0_in_all,\n",
        "        y1_in_all,\n",
        "        ate_in_all,\n",
        "        y0_out_all,\n",
        "        y1_out_all,\n",
        "        ate_out_all,\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1rUExSWWNOR",
        "outputId": "52d6796b-f6c8-4d10-ed3a-ef1fc6116ea3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------- Estimation of realization 0 via DML----------------------\n",
            "--------------------- Estimation of realization 1 via DML----------------------\n",
            "--------------------- Estimation of realization 2 via DML----------------------\n",
            "--------------------- Estimation of realization 3 via DML----------------------\n",
            "--------------------- Estimation of realization 4 via DML----------------------\n",
            "Saving ./estimation_results/IHDP-100/DML.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "WSwxt0PTyvSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mae_ATE(y0, y1, mu0, mu1):\n",
        "    \"\"\"\n",
        "    Calculate the mean absolute error of ATE estimation\n",
        "    :param y0: estimation of y0\n",
        "    :param y1: estimation of y1\n",
        "    :param mu0: ground-truth of potential outcome of T = 0\n",
        "    :param mu1: ground-truth of potential outcome of T = 1\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    ATE_gt = np.mean(mu1 - mu0)\n",
        "    ATE_pred = np.mean(y1 - y0)\n",
        "    return np.abs(ATE_gt - ATE_pred)\n",
        "\n",
        "\n",
        "def calculate_mae_ATEs(y0, y1, mu0, mu1):\n",
        "    \"\"\"\n",
        "    Calculate the mean absolute error of ATE estimation of\n",
        "    multiple realizations/datasets\n",
        "    :param y0: estimation of y0 of multiple realizations\n",
        "    :param y1: estimation of y1 of multiple realizations\n",
        "    :param mu0: ground-truth of y0 of multiple realizations\n",
        "    :param mu1: ground-truth of y1 of multiple realizations\n",
        "    :return: ndarray of MAE of ATE of all realizations\n",
        "    \"\"\"\n",
        "    assert y0.shape == mu0.shape, f\"shape of y0 and mu0: {y0.shape}{mu0.shape}\"\n",
        "    assert y1.shape == mu1.shape, f\"shape of y1 and mu1: {y1.shape}{mu1.shape}\"\n",
        "    assert y0.shape == y1.shape, f\"shape of y0 and y1: {y0.shape}{y1.shape}\"\n",
        "    num_real = y0.shape[-1]\n",
        "    mae_ATE = np.zeros(num_real)\n",
        "    for i in range(num_real):\n",
        "        mae_ATE[i] = calculate_mae_ATE(\n",
        "            y0[:, i], y1[:, i], mu0[:, i], mu1[:, i]\n",
        "        )\n",
        "    return mae_ATE\n",
        "\n",
        "def calculate_mae_ATEs_scalar(y0, y1, ate):\n",
        "    \"\"\"\n",
        "    Calculate the mean absolute error of ATE estimation of\n",
        "    multiple realizations/datasets\n",
        "    :param y0: estimation of y0 of multiple realizations\n",
        "    :param y1: estimation of y1 of multiple realizations\n",
        "    :param ate: ground-truth ate of multiple realizations\n",
        "    :return: ndarray of MAE of ATE of all realizations\n",
        "    \"\"\"\n",
        "    assert y0.shape == y1.shape, f\"shape of y0 and y1: {y0.shape}{y1.shape}\"\n",
        "    num_realizations = y0.shape[-1]\n",
        "    # each realization has an ate\n",
        "    if np.isscalar(ate) or ate.shape == (1,):\n",
        "        ate = ate * np.ones(num_realizations)\n",
        "    elif ate.shape == (1, 1):\n",
        "        ate = ate.item() * np.ones(num_realizations)\n",
        "    mae_ATE = np.zeros(num_realizations)\n",
        "    for i in range(num_realizations):\n",
        "        mae_ATE[i] = np.abs(np.mean(y1[:, i] - y0[:, i]) - ate[i])\n",
        "    return mae_ATE\n",
        "\n",
        "def calculate_PEHE(y0, y1, mu0, mu1):\n",
        "    \"\"\"\n",
        "    Calculate the Precision Estimation of Heterogeneous Effect (PEHE) of\n",
        "    one realization/dataset\n",
        "    :param y0: estimation of y0\n",
        "    :param y1: estimation of y1\n",
        "    :param mu0: ground-truth of potential outcome of T = 0\n",
        "    :param mu1: ground-truth of potential outcome of T = 1\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.sqrt(mean_squared_error(mu1 - mu0, y1 - y0))\n",
        "\n",
        "\n",
        "def calculate_PEHEs(y0, y1, mu0, mu1):\n",
        "    \"\"\"\n",
        "    Calculate the Precision Estimation of Heterogeneous Effect (PEHE) of\n",
        "    multiple realizations/datasets\n",
        "    :param y0: estimation of y0 of multiple realizations\n",
        "    :param y1: estimation of y1 of multiple realizations\n",
        "    :param mu0: ground-truth of y0 of multiple realizations\n",
        "    :param mu1: ground-truth of y1 of multiple realizations\n",
        "    \"\"\"\n",
        "    assert y0.shape == mu0.shape, f\"shape of y0 and mu0: {y0.shape}{mu0.shape}\"\n",
        "    assert y1.shape == mu1.shape, f\"shape of y1 and mu1: {y1.shape}{mu1.shape}\"\n",
        "    assert y0.shape == y1.shape, f\"shape of y0 and y1: {y0.shape}{y1.shape}\"\n",
        "    num_realizations = y0.shape[-1]\n",
        "    pehe = np.zeros(num_realizations)\n",
        "    for i in range(num_realizations):\n",
        "        pehe[i] = calculate_PEHE(y0[:, i], y1[:, i], mu0[:, i], mu1[:, i])\n",
        "    return pehe\n",
        "\n",
        "\n",
        "def calculate_metrics(y0, y1, ate, mu0, mu1, ate_gt, metric=\"PEHE\"):\n",
        "    if metric == \"PEHE\":\n",
        "        metric_over_realizations = calculate_PEHEs(y0, y1, mu0, mu1)\n",
        "    elif metric == \"MAE\":\n",
        "        if y0 is None or (y0.size == 1 and y0.item() == None):\n",
        "            metric_over_realizations = calculate_mae_ATEs_scalar(mu0, mu1, ate)\n",
        "        else:\n",
        "            metric_over_realizations = calculate_mae_ATEs(y0, y1, mu0, mu1)\n",
        "    return metric_over_realizations"
      ],
      "metadata": {
        "id": "2gPAtTAtQ_1G"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"IHDP-100\"\n",
        "estimator_set = [\n",
        "    \"Dragonnet\",\n",
        "    \"TARNet\",\n",
        "    \"DML\",\n",
        "]\n",
        "metrics_set = [\"PEHE\", \"MAE\"]\n",
        "num_realizations = 5\n",
        "print(f'{\" Evaluation \":-^79}')\n",
        "results_in: Dict[str, Dict] = {}\n",
        "results_out: Dict[str, Dict] = {}\n",
        "\n",
        "mu0_in, mu1_in, mu0_out, mu1_out = helper.load_IHDP_ground_truth(\n",
        "    datasets_folder, dataset_name, details=False\n",
        ")\n",
        "mu0_in, mu1_in =  mu0_in[:, :num_realizations], mu1_in[:, :num_realizations]\n",
        "mu0_out, mu1_out = mu0_out[:, :num_realizations], mu1_out[:, :num_realizations]\n",
        "ate_in_gt = np.mean(mu1_in - mu0_in)\n",
        "ate_out_gt = np.mean(mu1_out - mu0_out)\n",
        "\n",
        "for estimator_name in estimator_set:\n",
        "    estimation_result_folder = os.path.join(\n",
        "        results_folder, dataset_name, estimator_name\n",
        "    )\n",
        "    (\n",
        "        y0_in,\n",
        "        y1_in,\n",
        "        ate_in,\n",
        "        y0_out,\n",
        "        y1_out,\n",
        "        ate_out,\n",
        "    ) = helper.load_in_and_out_results(estimation_result_folder)\n",
        "    results_in[estimator_name] = {}\n",
        "    results_out[estimator_name] = {}\n",
        "    for metric in metrics_set:\n",
        "        if estimator_name == \"DML\" and metric == \"PEHE\":\n",
        "            results_in[estimator_name][metric] = {\"mean\": \"N/A\"}\n",
        "            results_out[estimator_name][metric] = {\"mean\": \"N/A\"}\n",
        "            continue\n",
        "\n",
        "        metric_in = calculate_metrics(\n",
        "            y0_in, y1_in, ate_in, mu0_in, mu1_in, ate_in_gt, metric=metric\n",
        "        )\n",
        "\n",
        "        results_in[estimator_name][metric] = {\n",
        "            \"mean\": np.mean(metric_in),\n",
        "        }\n",
        "\n",
        "        if estimator_name == \"DML\" and metric == \"PEHE\":\n",
        "            results_out[estimator_name][metric] = {\n",
        "                \"mean\": \"N/A\"\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        metric_out = calculate_metrics(\n",
        "            y0_out,\n",
        "            y1_out,\n",
        "            ate_out,\n",
        "            mu0_out,\n",
        "            mu1_out,\n",
        "            ate_out_gt,\n",
        "            metric=metric,\n",
        "        )\n",
        "\n",
        "        results_out[estimator_name][metric] = {\n",
        "            \"mean\": np.mean(metric_out)\n",
        "        }\n",
        "\n",
        "print(f'{\" In-sample results \":-^79}')\n",
        "for metric in metrics_set:\n",
        "    for estimator_name in estimator_set:\n",
        "        print(metric, estimator_name, results_in[estimator_name][metric])\n",
        "print(f'{\" Out-of-sample results \":-^79}')\n",
        "for metric in metrics_set:\n",
        "    for estimator_name in estimator_set:\n",
        "        print(metric, estimator_name, results_out[estimator_name][metric])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXSnUCPBQwj1",
        "outputId": "71f15193-b337-45f7-efb1-14c221b0b2f3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------- Evaluation ----------------------------------\n",
            "------------------------------ In-sample results ------------------------------\n",
            "PEHE Dragonnet {'mean': 0.6218022373142205}\n",
            "PEHE TARNet {'mean': 0.6401338339092958}\n",
            "PEHE DML {'mean': 'N/A'}\n",
            "MAE Dragonnet {'mean': 0.11109651623225555}\n",
            "MAE TARNet {'mean': 0.13775950879437798}\n",
            "MAE DML {'mean': 0.19793177791431624}\n",
            "---------------------------- Out-of-sample results ----------------------------\n",
            "PEHE Dragonnet {'mean': 0.6042200734424676}\n",
            "PEHE TARNet {'mean': 0.627264889527299}\n",
            "PEHE DML {'mean': 'N/A'}\n",
            "MAE Dragonnet {'mean': 0.1323210635143779}\n",
            "MAE TARNet {'mean': 0.16186412953915816}\n",
            "MAE DML {'mean': 0.17733408027474057}\n"
          ]
        }
      ]
    }
  ]
}