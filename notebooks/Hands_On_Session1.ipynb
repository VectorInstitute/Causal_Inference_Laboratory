{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VectorInstitute/Causal_Inference_Laboratory/blob/main/notebooks/Hands_On_Session1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWXJoMBul0VW"
      },
      "source": [
        "# Welocme to Hands-on Session #1\n",
        "\n",
        "---\n",
        "\n",
        "**Problem:** To estimate the causal impact of job training on unemployment\n",
        "\n",
        "---\n",
        "\n",
        "**Jobs Dataset**\n",
        "\n",
        "The Jobs by LaLonde (1986) is a widely used benchmark in the causal inference community, where the treatment is job training and the outcomes are binary outcomes of unemployment. This dataset combines a randomized study based on the National Supported Work program with observational data to form a larger dataset (Smith&Todd, 2005). The presence of the randomized subgroup gives a way to estimate the “groundtruth” causal effect. \n",
        "\n",
        "\n",
        "---\n",
        "**This notebook**\n",
        "Our dataset is the preprocessed jobs dataset (also available online https://www.fredjo.com/): it contains the LaLonde experimental sample (297 treated and 425 control) and the PSID comparison group (2490 control). The Jobs dataset is already split into the train/test (2570/642) splits in a 80/20 split.\n",
        "- X: There are 17 covariates such as age and education, as well as previous earnings.\n",
        "- T: Treatment\n",
        "- Y: Binary factual outcomes on unemployment\n",
        "---\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND9rTitGpFpp",
        "outputId": "b1d26ab0-ad53-4356-d85c-3577822154ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Causal_Inference_Laboratory'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (328/328), done.\u001b[K\n",
            "remote: Compressing objects: 100% (272/272), done.\u001b[K\n",
            "remote: Total 328 (delta 135), reused 212 (delta 51), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (328/328), 23.88 MiB | 10.92 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n",
            "Updating files: 100% (98/98), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/VectorInstitute/Causal_Inference_Laboratory.git\n",
        "!mv Causal_Inference_Laboratory code\n",
        "!mv code/data ./data\n",
        "!mv code/utils ./utils\n",
        "!mv code/models ./models\n",
        "!mv code/estimation_results ./estimation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYyfPzSok-1d"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKhGq-ljk-1g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from typing import Dict\n",
        "\n",
        "import utils.estimators as models\n",
        "import utils.preprocessing as helper\n",
        "from utils.preprocessing import sys_config\n",
        "import utils.metrics as metrics\n",
        "from utils.evaluation import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACFe3LMek-1i"
      },
      "outputs": [],
      "source": [
        "datasets_folder = sys_config[\"datasets_folder\"]\n",
        "results_folder = sys_config[\"results_folder\"]\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE8kkHWTz9zX"
      },
      "source": [
        "**Data Loading**\n",
        "\n",
        "In this module, we provide the data loading for the Jobs dataset. We will only use one realization of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-pxJ4fHk-1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5ea2aa-cb30-4a9d-cd25-7fae90447185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "The details of the train split of Jobs dataset:\n",
            "Number of realizations: 10\n",
            "ate (1, 1) float64\n",
            "e (2570, 10) float64\n",
            "I (2570, 10) int32\n",
            "yadd (1, 1) uint8\n",
            "yf (2570, 10) float64\n",
            "t (2570, 10) float64\n",
            "x (2570, 17, 10) float64\n",
            "ymul (1, 1) uint8\n",
            "-------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------\n",
            "The details of the test split of Jobs dataset:\n",
            "Number of realizations: 10\n",
            "ate (1, 1) float64\n",
            "e (642, 10) float64\n",
            "I (642, 10) int32\n",
            "yadd (1, 1) uint8\n",
            "yf (642, 10) float64\n",
            "t (642, 10) float64\n",
            "x (642, 17, 10) float64\n",
            "ymul (1, 1) uint8\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Let's load the data\n",
        "dataset_name = \"Jobs\"\n",
        "\n",
        "# Load covariates, treatment, and factual outcomes for the training and test datasets\n",
        "x_train_all, t_train_all, yf_train_all = helper.load_Jobs_observational(datasets_folder, dataset_name, details=True)\n",
        "x_test_all, t_test_all, yf_test_all = helper.load_Jobs_out_of_sample(datasets_folder, dataset_name, details=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will only use the first realization\n",
        "realization = 0\n",
        "x_train, t_train, yf_train = x_train_all[:, :, realization], t_train_all[:, realization], yf_train_all[:, realization]\n",
        "x_test, t_test, yf_test = x_test_all[:, :, realization], t_test_all[:, realization], yf_test_all[:, realization]\n",
        "\n",
        "print(x_train.shape, t_train.shape, yf_train.shape)\n",
        "print(x_test.shape, t_test.shape, yf_test.shape)\n",
        "\n",
        "t_value_train, count_t_train = np.unique(t_train, return_counts=True)\n",
        "yf_value_train, count_yf_train = np.unique(yf_train, return_counts=True)\n",
        "\n",
        "print(f\"Number of T = {int(t_value_train[0])} is {count_t_train[0]} and \\\n",
        "number of T = {int(t_value_train[1])} is {count_t_train[1]}.\")\n",
        "print(f\"Number of Y = {int(yf_value_train[0])} is {count_yf_train[0]} and \\\n",
        "number of Y = {int(yf_value_train[1])} is {count_yf_train[1]}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7dZDpNHtmii",
        "outputId": "9de792cf-5617-4a8e-8ea8-7830c3d319eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2570, 17) (2570,) (2570,)\n",
            "(642, 17) (642,) (642,)\n",
            "Number of T = 0 is 2333 and number of T = 1 is 237.\n",
            "Number of Y = 0 is 378 and number of Y = 1 is 2192.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PffuMCHCGbPK"
      },
      "source": [
        "In the Jobs dataset, though there is no ground-truth CATE as in IHDP, which is a synthetic dataset, we have ground-truth ATE. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CP6g84Pq7mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54cade1-91a2-46bf-8f28-16c01836b4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07794018617548037 0.07794018617548037\n"
          ]
        }
      ],
      "source": [
        "# Load Ground Truth facutal and counterfactual outcomes for the training and test datasets\n",
        "ate_in_gt, ate_out_gt = helper.load_Jobs_ground_truth(datasets_folder, dataset_name, details=False)\n",
        "ate_in_gt, ate_out_gt = ate_in_gt.item(), ate_out_gt.item()\n",
        "print(ate_in_gt, ate_out_gt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation\n",
        "Now you can build causal estimators to measure the impact of training on unemployment given all the available $(X, T, YF)$ tuples. \n",
        "\n",
        "A few reminders:\n",
        "- we have already done the train-test split (80/20 split) for you; for further splitting, a train/validation/test split with ratios 56/24/20 is used in the TAR-Net paper.\n",
        "- the outcomes are binary values.\n",
        "- we have the ground-truth ATE.\n"
      ],
      "metadata": {
        "id": "mgOn11fI5uaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given \n",
        "# x_train, t_train, yf_train \n",
        "# x_test, t_test, yf_test\n",
        "# ate_in_gt, ate_out_gt"
      ],
      "metadata": {
        "id": "-ObWhSB407TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: S-learner\n",
        "In this part, you are asked to build any S-learner (viewing t as one feature)."
      ],
      "metadata": {
        "id": "GqZLTwdfuUr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_slearner(x, t, yf, x_t):\n",
        "    \"\"\"\n",
        "    Training a s-leaner\n",
        "    :param x: covariates\n",
        "    :param t: treatment\n",
        "    :param yf: factual outcomes\n",
        "    :param x_t: out-of-sample covariates\n",
        "    :return:\n",
        "    \"\"\"    \n",
        "    ####################\n",
        "    #PUT YOUR CODE HERE#\n",
        "    ####################\n",
        "    return ate_in, ate_out\n",
        "\n"
      ],
      "metadata": {
        "id": "5xVrQZXm27Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: T-learner\n",
        "In this part, you are asked to build any T-learner (building separate models for different treatment groups)."
      ],
      "metadata": {
        "id": "GPh5Dn4yDPev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_tlearner(x, t, yf, x_t):\n",
        "    \"\"\"\n",
        "    Training a t-leaner\n",
        "    :param x: covariates\n",
        "    :param t: treatment\n",
        "    :param yf: factual outcomes\n",
        "    :param x_t: out-of-sample covariates\n",
        "    :return:\n",
        "    \"\"\"    \n",
        "    ####################\n",
        "    #PUT YOUR CODE HERE#\n",
        "    ####################\n",
        "    return ate_in, ate_out"
      ],
      "metadata": {
        "id": "YfmNuGzF450W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: Deep Learning-based learner\n",
        "In this part, you are asked to build any estimator like TAR-Net/Dragonnet."
      ],
      "metadata": {
        "id": "l9t2D10yDQ79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_dllearner(x, t, yf, x_t):\n",
        "    \"\"\"\n",
        "    Training a deep learning-based leaner\n",
        "    :param x: covariates\n",
        "    :param t: treatment\n",
        "    :param yf: factual outcomes\n",
        "    :param x_t: out-of-sample covariates\n",
        "    :return:\n",
        "    \"\"\"    \n",
        "    ####################\n",
        "    #PUT YOUR CODE HERE#\n",
        "    ####################\n",
        "    return ate_in, ate_out"
      ],
      "metadata": {
        "id": "aogLt2uY5fji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4: IPW\n",
        "In this part, you are asked to build an estimator based on Inverse Propensity Weighting."
      ],
      "metadata": {
        "id": "Xeu-CNZM2Y-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_ipw(x, t, yf, x_t):\n",
        "    \"\"\"\n",
        "    Training a deep learning-based leaner\n",
        "    :param x: covariates\n",
        "    :param t: treatment\n",
        "    :param yf: factual outcomes\n",
        "    :param x_t: out-of-sample covariates\n",
        "    :return:\n",
        "    \"\"\"    \n",
        "    ####################\n",
        "    #PUT YOUR CODE HERE#\n",
        "    ####################\n",
        "    return ate_in, ate_out"
      ],
      "metadata": {
        "id": "vTpmeWYi5pWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5: Double Machine Learning\n",
        "In this part, you are asked to build any R-learner (Double machine learning estimator)."
      ],
      "metadata": {
        "id": "e7qbGCGt2hAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_dml(x, t, yf, x_t):\n",
        "    \"\"\"\n",
        "    Training a double machine learning leaner\n",
        "    :param x: covariates\n",
        "    :param t: treatment\n",
        "    :param yf: factual outcomes\n",
        "    :param x_t: out-of-sample covariates\n",
        "    :return:\n",
        "    \"\"\"    \n",
        "    ####################\n",
        "    #PUT YOUR CODE HERE#\n",
        "    ####################\n",
        "    return ate_in, ate_out"
      ],
      "metadata": {
        "id": "uWzkykO35tTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "NDMgz1md2ZIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mae_ATE(ate, ate_gt):\n",
        "    \"\"\"\n",
        "    Calculate the absolute error of ATE estimation\n",
        "    :param ate: predicted ate\n",
        "    :param ate_gt: ground-truth ate\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.abs(ate - ate_gt)\n",
        "\n",
        "estimator_set = [\"S-Learner\", \"T-Learner\", \"DL-Leaner\", \"IPW\", \"DML\"]\n",
        "\n",
        "for estimator in estimator_set:\n",
        "    if estimator == \"S-Learner\":\n",
        "        ate_in, ate_out = train_and_evaluate_slearner(x_train, t_train, yf_train, x_test)\n",
        "    elif estimator == \"T-Learner\":\n",
        "        ate_in, ate_out = train_and_evaluate_tlearner(x_train, t_train, yf_train, x_test)\n",
        "    elif estimator == \"DL-Learner\":\n",
        "        ate_in, ate_out = train_and_evaluate_dllearner(x_train, t_train, yf_train, x_test)\n",
        "    elif estimator == \"IPW\":\n",
        "        ate_in, ate_out = train_and_evaluate_slearner(x_train, t_train, yf_train, x_test)\n",
        "    elif estimator == \"DML\":\n",
        "        ate_in, ate_out = train_and_evaluate_slearner(x_train, t_train, yf_train, x_test)\n",
        "    else:\n",
        "        print(\"Undefined estimator\")\n",
        "        ate_in, ate_out = None, None\n",
        "\n",
        "    text = f\" Estimation via {estimator}\"\n",
        "    print(f\"{text:-^79}\")\n",
        "    # in-sample\n",
        "    print(f\"Absolute error of in-sample ATE of {estimator}: {calculate_mae_ATE(ate_in, ate_in_gt) if ate_in is not None else 'N/A'}\")\n",
        "\n",
        "    # out-of-sample\n",
        "    print(f\"Absolute error of out-of-sample ATE of {estimator}: {calculate_mae_ATE(ate_out, ate_out_gt) if ate_in is not None else 'N/A'}\")"
      ],
      "metadata": {
        "id": "nDqLI_Zz5zpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection"
      ],
      "metadata": {
        "id": "sZziAgCWDSS8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a0b5a31022a61b32d9bd73ec997b0a079a20f27350e8c7766473b50790ea745"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}