{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VectorInstitute/Causal_Inference_Laboratory/blob/main/notebooks/estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation\n",
        "\n",
        "## Upload Code\n",
        "\n",
        "Run this code to clone the repository and prepare it. \n"
      ],
      "metadata": {
        "id": "WWXJoMBul0VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VectorInstitute/Causal_Inference_Laboratory.git\n",
        "!mv Causal_Inference_Laboratory code\n",
        "!mv code/data ./data\n",
        "!mv code/utils ./utils\n",
        "!mv code/models ./models\n",
        "!mv code/estimation_results ./estimation_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND9rTitGpFpp",
        "outputId": "1a7243a3-c9a5-4bc1-8c06-5e206f2af4df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Causal_Inference_Laboratory'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 120 (delta 17), reused 109 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (120/120), 14.93 MiB | 22.59 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost==1.3.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtP3r5ybskjO",
        "outputId": "f74e01eb-58c3-4427-f482-dc90a116984f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xgboost==1.3.3\n",
            "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.3.3) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.3.3) (1.10.1)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 1.7.5\n",
            "    Uninstalling xgboost-1.7.5:\n",
            "      Successfully uninstalled xgboost-1.7.5\n",
            "Successfully installed xgboost-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYyfPzSok-1d"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKhGq-ljk-1g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import utils.estimators as models\n",
        "import utils.preprocessing as helper\n",
        "from utils.preprocessing import sys_config\n",
        "import utils.metrics as metrics\n",
        "from utils.evaluation import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACFe3LMek-1i"
      },
      "outputs": [],
      "source": [
        "datasets_folder = sys_config[\"datasets_folder\"]\n",
        "results_folder = sys_config[\"results_folder\"]\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNPhsTZVk-1i"
      },
      "source": [
        "# Description of datasets\n",
        "We briefly discuss the datasets here.\n",
        "\n",
        "## IHDP\n",
        "Infant Health and Development Program (IHDP) [1] is from a\n",
        "randomized experiment studying the effect of home visits by specialists on\n",
        "future cognitive test scores of children. The children of non-white mothers in\n",
        "the treated set are removed to de-randomize the experiment. Each unit is\n",
        "simulated for a treated and a control outcome (so we know the ground-truth of\n",
        "the individual treatment effects).\n",
        "\n",
        "\n",
        "\n",
        "The IHDP datasets are already split into the train (672 for each realization)\n",
        "and test (75 for each realization) splits in a 90/10 split. Each `.npz` file\n",
        "contains the following keys: x, t, yf, ycf, mu0, mu1, which are respectively\n",
        "covariates, treatment, factual outcome, counterfactual outcome, noiseless\n",
        "potential control outcome, and noiseless potential treated outcome.\n",
        "- IHDP-100: 100 realizations of the IHDP dataset (included in our repo);\n",
        "- IHDP-1000: 1000 realizations of the IHDP dataset\n",
        "(downloadable from https://www.fredjo.com/);\n",
        "\n",
        "## Jobs\n",
        "Jobs is a dataset derived from LaLonde [2] where the original data set has job\n",
        "training as the treatment and income and employment status after training as\n",
        "outcomes. The Jobs dataset is proposed in [3] using the LaLonde experimental\n",
        "sample (297 treated, 425 control) and the PSID comparison group (2490 control).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The Jobs datasets are already split into the train (2570 for each realization)\n",
        "and test (642 for each realization) splits in a 80/20 split. Each `.npz` file\n",
        "contains the following keys: x, t, yf, ate, which are respectively\n",
        "covariates, treatment, factual outcome, and average treatment effect (scalar).\n",
        "- Jobs: 10 realizations of the Jobs dataset (included in our repo);\n",
        "\n",
        "## Twins\n",
        "\n",
        "TWINS [4]. The dataset is from the data of twin births in the USA between 1989-1991 [5] about the effect of the relative weight of each of the twins on the morality of them. The treatment is whether the twin is born heavier than the other twin (T = 1 means heavier) and the outcomes are the first-year mortality of the twins. It has 23968 units (11984 treated, 11984 control) and 46 covariates relating to the parents, the pregnancy and birth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-pxJ4fHk-1j"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"IHDP-100\" #@param [\"IHDP-100\", \"Jobs\", \"TWINS\"]\n",
        "if dataset_name == \"Jobs\":\n",
        "  x_all, t_all, yf_all = helper.load_Jobs_observational(\n",
        "              datasets_folder, dataset_name, details=False\n",
        "          )\n",
        "\n",
        "  x_test_all = helper.load_Jobs_out_of_sample(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "elif dataset_name == \"TWINS\":\n",
        "  x_all, t_all, yf_all = helper.load_TWINS_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "  x_test_all, t_test_all = helper.load_TWINS_out_of_sample(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "elif dataset_name == \"IHDP-100\":\n",
        "  x_all, t_all, yf_all = helper.load_IHDP_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "  x_test_all = helper.load_IHDP_out_of_sample(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1_a0EeWk-1l"
      },
      "source": [
        "## Estimation\n",
        "\n",
        "### DML\n",
        "\n",
        "The double machine learning method works as follows. We first partition the data set into $2$ subsets $I$ and $I^{c}$. Then train any model $M_{t}$ to estimate $T$ from $X$ using $I^{c}$ and train any model $M_{y}$ to estimate $Y$ from $X$ using $I^{c}$. Calculate the residuals $Y_{R} = Y - M_{y}(X)$ and $T_{R} = T - M_{t}(X)$ on $I$. Regress $Y_{R}$ on $T_{R}$ to get the estimated ATE.\n",
        "\n",
        "### IPW\n",
        "\n",
        "Define the propensity score as the probability of a unit with covariate $x$ to be treated by $T=1$, i.e., $\\pi(x) = \\mathbb{P}(T=1| X=x)$. We define the IPW outcomes as:\n",
        "\\begin{align*}\n",
        "    Y_{i}(1)^{\\text{IPW}} = Y_{i}(1) \\frac{\\mathbb{1}(T=1)}{\\pi_{i}(x)},\\quad Y_{i}(0)^{\\text{IPW}} = Y_{i}(0) \\frac{\\mathbb{1}(T=0)}{1-\\pi_{i}(x)}\n",
        "\\end{align*}\n",
        "If the true propensity is known, it is can be easily verified that $\\mathbb{E}[Y_{i}(1)^{\\text{IPW}}] = Y_{i}(1)$ and $\\mathbb{E}[Y_{i}(0)^{\\text{IPW}}] = Y_{i}(0)$, i.e., the IPW outcomes are the unbiased estimator of each individual's potential outcomes. \n",
        "\n",
        "The inverse propensity weighted (IPW) estimator models the propensity score instead of modeling the potential outcomes. However, since the propensity score is in the denominator, this approach tends to have very high variance.\n",
        "\n",
        "### OLS1\n",
        "\n",
        "TBC.\n",
        "\n",
        "### OLS2\n",
        "\n",
        "TBC.\n",
        "\n",
        "### NN1\n",
        "\n",
        "TBC.\n",
        "\n",
        "### NN2\n",
        "\n",
        "TBC.\n",
        "\n",
        "### RF1\n",
        "\n",
        "TBC.\n",
        "\n",
        "### RF2\n",
        "\n",
        "TBC.\n",
        "\n",
        "### Dragonnet\n",
        "\n",
        "TBC.\n",
        "\n",
        "### TARNet\n",
        "\n",
        "TAR-Net [3] is proposed to overcome the problem of potentially ignoring $T$ when $X$ is high dimensional in COM and the problem of inefficient data usage as units are split into groups in GCOM. It is able to utilize all units through a common embedding and uses two heads for the two treatments.\n",
        "\n",
        "Run and save estimatotion results using a specified estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-zbX9PZk-1l"
      },
      "outputs": [],
      "source": [
        "def estimate(estimator_name):\n",
        "    num_realizations = x_all.shape[-1]\n",
        "    print(\"Numer of realizations:\", num_realizations)\n",
        "    y0_in_all, y1_in_all, y0_out_all, y1_out_all = [], [], [], []\n",
        "    ate_in_all, ate_out_all = [], []\n",
        "    for i in range(num_realizations):\n",
        "        text = f\" Estimation of realization {i} via {estimator_name}\"\n",
        "        print(f\"{text:-^79}\")\n",
        "        x, t, yf = x_all[:, :, i], t_all[:, i], yf_all[:, i]\n",
        "        x_test = x_test_all[:, :, i]\n",
        "        # train the estimator and predict for this realization\n",
        "        (\n",
        "            y0_in,\n",
        "            y1_in,\n",
        "            ate_in,\n",
        "            y0_out,\n",
        "            y1_out,\n",
        "            ate_out,\n",
        "        ) = models.train_and_evaluate(x, t, yf, x_test, estimator_name)\n",
        "        y0_in_all.append(y0_in)\n",
        "        y1_in_all.append(y1_in)\n",
        "        ate_in_all.append(ate_in)\n",
        "        y0_out_all.append(y0_out)\n",
        "        y1_out_all.append(y1_out)\n",
        "        ate_out_all.append(ate_out)\n",
        "    # follow the dimension order of the dataset,\n",
        "    # i.e., realizations are captured by the last index\n",
        "    y0_in_all = np.squeeze(np.array(y0_in_all).transpose()).reshape((-1, num_realizations))\n",
        "    y1_in_all = np.squeeze(np.array(y1_in_all).transpose()).reshape((-1, num_realizations))\n",
        "    y0_out_all = np.squeeze(np.array(y0_out_all).transpose()).reshape((-1, num_realizations))\n",
        "    y1_out_all = np.squeeze(np.array(y1_out_all).transpose()).reshape((-1, num_realizations))\n",
        "    ate_in_all = np.array(ate_in_all).reshape((num_realizations,))\n",
        "    ate_out_all = np.array(ate_out_all).reshape((num_realizations,))\n",
        "\n",
        "    # save estimation results\n",
        "    estimation_result_folder = os.path.join(\n",
        "        results_folder, dataset_name, estimator_name\n",
        "    )\n",
        "    print(f\"Saving {estimation_result_folder}.\")\n",
        "    helper.save_in_and_out_results(\n",
        "        estimation_result_folder,\n",
        "        y0_in_all,\n",
        "        y1_in_all,\n",
        "        ate_in_all,\n",
        "        y0_out_all,\n",
        "        y1_out_all,\n",
        "        ate_out_all,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = \"Dragonnet\" #@param estimator_set = [\"IPW\", \"OLS1\", \"OLS2\", \"NN1\", \"NN2\", \"RF1\", \"RF2\", \"Dragonnet\", \"TARNet\"]\n",
        "\n",
        "\n",
        "estimate(estimator)"
      ],
      "metadata": {
        "id": "FK41I9biBEa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmAydp3Vk-1o"
      },
      "source": [
        "## Evalutation\n",
        "\n",
        "TODO: description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaTQoY0Ik-1o"
      },
      "outputs": [],
      "source": [
        "def evaluate(estimator_name, metrics_set):\n",
        "    print(f'{\" Evaluation \":-^79}')\n",
        "    results_in = {}\n",
        "    results_out = {}\n",
        "    if dataset_name == \"Jobs\":\n",
        "        ate_in_gt, ate_out_gt = helper.load_Jobs_ground_truth(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        mu0_in, mu1_in, mu0_out, mu1_out = None, None, None, None\n",
        "        x_all, t_all, yf_all = helper.load_Jobs_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        x_test_all = helper.load_Jobs_out_of_sample(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "    elif \"IHDP\" in dataset_name:\n",
        "        mu0_in, mu1_in, mu0_out, mu1_out = helper.load_IHDP_ground_truth(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        ate_in_gt = np.mean(mu1_in - mu0_in)\n",
        "        ate_out_gt = np.mean(mu1_out - mu0_out)\n",
        "        x_all, t_all, yf_all = helper.load_IHDP_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        x_test_all = helper.load_IHDP_out_of_sample(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "    elif dataset_name == \"TWINS\":\n",
        "        mu0_in, mu1_in, mu0_out, mu1_out = helper.load_TWINS_ground_truth(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        ate_in_gt = np.mean(mu1_in - mu0_in)\n",
        "        ate_out_gt = np.mean(mu1_out - mu0_out)\n",
        "        x_all, t_all, yf_all = helper.load_TWINS_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        x_test_all, t_test_all = helper.load_TWINS_out_of_sample(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        mu0_in, mu1_in, mu0_out, mu1_out = helper.load_TWINS_ground_truth(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "\n",
        "    indices_all = np.arange(x_all.shape[0])\n",
        "\n",
        "    x_train, x_eval, t_train, t_eval, yf_train, yf_eval, inidices_train, indices_eval = train_test_split(\n",
        "        x_all, t_all, yf_all, indices_all, test_size=0.2, random_state=seed\n",
        "        )\n",
        "    \n",
        "    data_size = x_eval.shape[0]\n",
        "    num_realizations = 1\n",
        "    if len(x_eval.shape) == 3:\n",
        "        num_realizations = x_eval.shape[2]\n",
        "        data_size = x_eval.shape[0] * x_eval.shape[2]\n",
        "\n",
        "    \n",
        "    # squeeze all eval data\n",
        "    x_eval = np.reshape(x_eval, (data_size, x_eval.shape[1]))\n",
        "    t_eval = np.reshape(t_eval, (data_size))\n",
        "    yf_eval = np.reshape(yf_eval, (data_size))\n",
        "\n",
        "    #Computing relevant evaluation metric for ensemble\n",
        "    nuisance_stats_dir= results_folder + '//..//models//' + dataset_name + '//'\n",
        "    # Nuisance Models\n",
        "    prop_prob, prop_score = get_nuisance_propensity_pred(x_eval, t_eval, save_dir=nuisance_stats_dir)\n",
        "    outcome_s_pred = get_nuisance_outome_s_pred(x_eval, t_eval, save_dir=nuisance_stats_dir)\n",
        "    outcome_t_pred = get_nuisance_outcome_t_pred(x_eval, t_eval, save_dir=nuisance_stats_dir)\n",
        "    outcome_r_pred = get_nuisance_outcome_r_pred(x_eval, save_dir=nuisance_stats_dir)\n",
        "\n",
        "    estimation_result_folder = os.path.join(\n",
        "        results_folder, dataset_name, estimator_name\n",
        "    )\n",
        "    (\n",
        "        y0_in,\n",
        "        y1_in,\n",
        "        ate_in,\n",
        "        y0_out,\n",
        "        y1_out,\n",
        "        ate_out,\n",
        "    ) = helper.load_in_and_out_results(estimation_result_folder)\n",
        "    \n",
        "    if dataset_name == \"TWINS\":\n",
        "        y0_in = y0_in.reshape((-1, 1))\n",
        "        y1_in = y1_in.reshape((-1, 1))\n",
        "        y0_out = y0_out.reshape((-1, 1))\n",
        "        y1_out = y1_out.reshape((-1, 1))\n",
        "        ate_in = ate_in.reshape((-1, 1))\n",
        "        ate_out = ate_out.reshape((-1, 1))\n",
        "            \n",
        "    results_in[estimator_name] = {}\n",
        "    results_out[estimator_name] = {}\n",
        "\n",
        "    for metric in metrics_set:\n",
        "        metric_in = None\n",
        "        ite_estimate_eval = (y1_in[indices_eval] - y0_in[indices_eval])\n",
        "        if metric in [\"MAE\", \"PEHE\"]:\n",
        "            metric_in = metrics.calculate_metrics(\n",
        "                y0_in, y1_in, ate_in, mu0_in, mu1_in, ate_in_gt, metric=metric\n",
        "            )\n",
        "        elif metric == \"value_score\":\n",
        "            metric_in = metrics.calculate_value_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, dataset_name=dataset_name, prop_score=prop_score\n",
        "            )\n",
        "        elif metric == \"value_dr_score\":\n",
        "            metric_in = metrics.calculate_value_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, dataset_name=dataset_name, prop_score=prop_score\n",
        "            )\n",
        "        elif metric == \"value_dr_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_value_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, dataset_name=dataset_name, prop_score=prop_score, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"tau_match_score\":\n",
        "            metric_in = metrics.calculate_tau_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval\n",
        "            )\n",
        "        elif metric == \"tau_iptw_score\":\n",
        "            metric_in = metrics.calculate_tau_iptw_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, prop_score=prop_score\n",
        "            )\n",
        "        elif metric == \"tau_iptw_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_tau_iptw_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, prop_score=prop_score, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"tau_dr_score\":\n",
        "            metric_in = metrics.calculate_tau_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_score=prop_score\n",
        "            )\n",
        "        elif metric == \"tau_dr_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_tau_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_score=prop_score, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"tau_s_score\":\n",
        "            metric_in = metrics.calculate_tau_s_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_s_pred\n",
        "            )\n",
        "        elif metric == \"tau_t_score\":\n",
        "            metric_in = metrics.calculate_tau_t_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred\n",
        "            )\n",
        "        elif metric == \"influence_score\":\n",
        "            metric_in = metrics.calculate_influence_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_prob=prop_prob\n",
        "            )\n",
        "        elif metric == \"influence_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_influence_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_prob=prop_prob, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"r_score\":\n",
        "            metric_in = metrics.calculate_r_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_r_pred, treatment_prob=prop_prob[:, 1]\n",
        "            )\n",
        "\n",
        "        if metric_in is None:\n",
        "            results_in[estimator_name][metric] = {\"mean\": None}\n",
        "        else:\n",
        "            results_in[estimator_name][metric] = {\n",
        "                \"mean\": np.mean(metric_in, where=(metric_in != 0)),\n",
        "            }\n",
        "\n",
        "    print(f'{\" In-sample results \":-^79}')\n",
        "    for metric in metrics_set:\n",
        "        print(metric, estimator_name, results_in[estimator_name][metric])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = \"value_dr_score\" #@param [ \"MAE\", \"PEHE\", \"value_score\", \"value_dr_score\", \"value_dr_clip_prop_score\", \"tau_t_score\", \"tau_s_score\", \"tau_match_score\", \"tau_iptw_score\", \"tau_iptw_clip_prop_score\", \"tau_dr_score\", \"tau_dr_clip_prop_score\", \"influence_score\", \"influence_clip_prop_score\", \"r_score\", \"ALL\"]\n",
        "if metric == \"ALL\":\n",
        "  metric_set = [ \"MAE\", \"PEHE\", \"value_score\", \"value_dr_score\", \"value_dr_clip_prop_score\", \"tau_t_score\", \"tau_s_score\", \"tau_match_score\", \"tau_iptw_score\", \"tau_iptw_clip_prop_score\", \"tau_dr_score\", \"tau_dr_clip_prop_score\", \"influence_score\", \"influence_clip_prop_score\", \"r_score\"]\n",
        "else:\n",
        "  metric_set = [metric]\n",
        "\n",
        "evaluate(estimator, metric_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkyNYLSwdVeW",
        "outputId": "e3965e69-3a16-422c-8724-607cf4a63d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------- Evaluation ----------------------------------\n",
            "------------------------------ In-sample results ------------------------------\n",
            "value_dr_score Dragonnet {'mean': nan}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "[1] J. L. Hill, “Bayesian nonparametric modeling for causal inference,” Journal\n",
        "of Computational  and Graphical Statistics, vol. 20, no. 1, pp. 217–240, 2011.\n",
        "[Online]. Available: https://doi.org/10.1198/jcgs.2010.08162\n",
        "\n",
        "[2] R. J. LaLonde, “Evaluating the econometric evaluations of training programs\n",
        "with experimental data,” The American Economic Review, vol. 76, no. 4, pp.\n",
        "604–620, 1986. [Online]. Available: http://www.jstor.org/stable/1806062\n",
        "\n",
        "[3] U. Shalit, F. D. Johansson, and D. Sontag, “Estimating individual treatment\n",
        "effect: generalization bounds and algorithms,” in Proceedings of the 34th\n",
        "International Conference on Machine Learning, ser. Proceedings of Machine\n",
        "Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70. PMLR, 06–11 Aug 2017\n",
        ", pp. 3076–3085. [Online].\n",
        "Available: https://proceedings.mlr.press/v70/shalit17a.html\n",
        "\n",
        "[4] Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17), 6449–6459, 2017.\n",
        "\n",
        "[5] D. Almond, K. Y. Chay, and D. S. Lee. The costs of low birth weight.The Quarterly Journal of Economics,120(3):1031–1083, 2005.\n"
      ],
      "metadata": {
        "id": "6LqqReqIlQZg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a0b5a31022a61b32d9bd73ec997b0a079a20f27350e8c7766473b50790ea745"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}