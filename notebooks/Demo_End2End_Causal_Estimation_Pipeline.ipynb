{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VectorInstitute/Causal_Inference_Laboratory/blob/main/notebooks/Demo_End2End_Causal_Estimation_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation\n",
        "\n",
        "## Upload Code\n",
        "\n",
        "Run this code to clone the repository and prepare it.\n"
      ],
      "metadata": {
        "id": "WWXJoMBul0VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VectorInstitute/Causal_Inference_Laboratory.git\n",
        "!mv Causal_Inference_Laboratory code\n",
        "!mv code/data ./data\n",
        "!mv code/utils ./utils\n",
        "!mv code/models ./models\n",
        "!mv code/estimation_results ./estimation_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND9rTitGpFpp",
        "outputId": "78284ed0-b3a4-4650-fb50-7968e2cdc907"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Causal_Inference_Laboratory'...\n",
            "remote: Enumerating objects: 1118, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 1118 (delta 23), reused 50 (delta 20), pack-reused 1059\u001b[K\n",
            "Receiving objects: 100% (1118/1118), 178.16 MiB | 10.61 MiB/s, done.\n",
            "Resolving deltas: 100% (343/343), done.\n",
            "Updating files: 100% (413/413), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost==1.3.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtP3r5ybskjO",
        "outputId": "ac5b3153-f5ed-45b6-b5d1-69fcba3bc54b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==1.3.3\n",
            "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.3.3) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.3.3) (1.10.1)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 1.7.6\n",
            "    Uninstalling xgboost-1.7.6:\n",
            "      Successfully uninstalled xgboost-1.7.6\n",
            "Successfully installed xgboost-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYyfPzSok-1d"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zKhGq-ljk-1g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import utils.estimators as models\n",
        "import utils.preprocessing as helper\n",
        "from utils.preprocessing import sys_config\n",
        "import utils.metrics as metrics\n",
        "from utils.evaluation import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ACFe3LMek-1i"
      },
      "outputs": [],
      "source": [
        "datasets_folder = sys_config[\"datasets_folder\"]\n",
        "results_folder = sys_config[\"results_folder\"]\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNPhsTZVk-1i"
      },
      "source": [
        "# Description of datasets\n",
        "We briefly discuss the datasets here.\n",
        "\n",
        "## Jobs\n",
        "Jobs is a dataset derived from LaLonde [2] where the original data set has job\n",
        "training as the treatment and income and employment status after training as\n",
        "outcomes. The Jobs dataset is proposed in [3] using the LaLonde experimental\n",
        "sample (297 treated, 425 control) and the PSID comparison group (2490 control).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The Jobs datasets are already split into the train (2570 for each realization)\n",
        "and test (642 for each realization) splits in a 80/20 split. Each `.npz` file\n",
        "contains the following keys: x, t, yf, ate, which are respectively\n",
        "covariates, treatment, factual outcome, and average treatment effect (scalar).\n",
        "- Jobs: 10 realizations of the Jobs dataset (included in our repo);\n",
        "\n",
        "## Twins\n",
        "\n",
        "TWINS [4]. The dataset is from the data of twin births in the USA between 1989-1991 [5] about the effect of the relative weight of each of the twins on the morality of them. The treatment is whether the twin is born heavier than the other twin (T = 1 means heavier) and the outcomes are the first-year mortality of the twins. It has 23968 units (11984 treated, 11984 control) and 46 covariates relating to the parents, the pregnancy and birth.\n",
        "\n",
        "## IHDP\n",
        "Infant Health and Development Program (IHDP) [1] is from a\n",
        "randomized experiment studying the effect of home visits by specialists on\n",
        "future cognitive test scores of children. The children of non-white mothers in\n",
        "the treated set are removed to de-randomize the experiment. Each unit is\n",
        "simulated for a treated and a control outcome (so we know the ground-truth of\n",
        "the individual treatment effects).\n",
        "\n",
        "The IHDP datasets are already split into the train (672 for each realization)\n",
        "and test (75 for each realization) splits in a 90/10 split. Each `.npz` file\n",
        "contains the following keys: x, t, yf, ycf, mu0, mu1, which are respectively\n",
        "covariates, treatment, factual outcome, counterfactual outcome, noiseless\n",
        "potential control outcome, and noiseless potential treated outcome.\n",
        "- IHDP-100: 100 realizations of the IHDP dataset (included in our repo);\n",
        "- IHDP-1000: 1000 realizations of the IHDP dataset\n",
        "(downloadable from https://www.fredjo.com/);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j-pxJ4fHk-1j"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"TWINS\" #@param [\"Jobs\", \"TWINS\", \"IHDP-100\"]\n",
        "if dataset_name == \"Jobs\":\n",
        "  x_all, t_all, yf_all = helper.load_Jobs_observational(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "  x_test_all, t_test_all, yf_test_all = helper.load_Jobs_out_of_sample(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "elif dataset_name == \"TWINS\":\n",
        "  x_all, t_all, yf_all = helper.load_TWINS_observational(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "  x_test_all, t_test_all, yf_test_all = helper.load_TWINS_out_of_sample(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "elif dataset_name == \"IHDP-100\":\n",
        "  x_all, t_all, yf_all = helper.load_IHDP_observational(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "  x_test_all, t_test_all, yf_test_all = helper.load_IHDP_out_of_sample(\n",
        "      datasets_folder, dataset_name, details=False\n",
        "  )\n",
        "  yf_all = helper.scale_y(yf_all)\n",
        "  yf_test_all = helper.scale_y(yf_test_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1_a0EeWk-1l"
      },
      "source": [
        "## Estimation\n",
        "\n",
        "The estimators available are:\n",
        "- COM/S-Learner: OLS1, RF1, NN1;\n",
        "- GCOM/T-Learner: OLS2, RF2, NN2;\n",
        "- TARNet\n",
        "- Dragonnet\n",
        "- DML\n",
        "\n",
        "Run and save estimatotion results using a specified estimator. Pre-trained estimation results are already provided, so to save time, you can skip the 2 cells below and proceed to Evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t-zbX9PZk-1l"
      },
      "outputs": [],
      "source": [
        "def estimate(estimator_name):\n",
        "    num_realizations = x_all.shape[-1]\n",
        "    print(\"Numer of realizations:\", num_realizations)\n",
        "    y0_in_all, y1_in_all, y0_out_all, y1_out_all = [], [], [], []\n",
        "    ate_in_all, ate_out_all = [], []\n",
        "    for i in range(num_realizations):\n",
        "        text = f\" Estimation of realization {i} via {estimator_name}\"\n",
        "        print(f\"{text:-^79}\")\n",
        "        x, t, yf = x_all[:, :, i], t_all[:, i], yf_all[:, i]\n",
        "        x_test = x_test_all[:, :, i]\n",
        "        # train the estimator and predict for this realization\n",
        "        (\n",
        "            y0_in,\n",
        "            y1_in,\n",
        "            ate_in,\n",
        "            y0_out,\n",
        "            y1_out,\n",
        "            ate_out,\n",
        "        ) = models.train_and_evaluate(x, t, yf, x_test, estimator_name, dataset_name)\n",
        "        y0_in_all.append(y0_in)\n",
        "        y1_in_all.append(y1_in)\n",
        "        ate_in_all.append(ate_in)\n",
        "        y0_out_all.append(y0_out)\n",
        "        y1_out_all.append(y1_out)\n",
        "        ate_out_all.append(ate_out)\n",
        "    # follow the dimension order of the dataset,\n",
        "    # i.e., realizations are captured by the last index\n",
        "    y0_in_all = np.squeeze(np.array(y0_in_all).transpose()).reshape((-1, num_realizations))\n",
        "    y1_in_all = np.squeeze(np.array(y1_in_all).transpose()).reshape((-1, num_realizations))\n",
        "    y0_out_all = np.squeeze(np.array(y0_out_all).transpose()).reshape((-1, num_realizations))\n",
        "    y1_out_all = np.squeeze(np.array(y1_out_all).transpose()).reshape((-1, num_realizations))\n",
        "    ate_in_all = np.array(ate_in_all).reshape((num_realizations,))\n",
        "    ate_out_all = np.array(ate_out_all).reshape((num_realizations,))\n",
        "\n",
        "    # save estimation results\n",
        "    estimation_result_folder = os.path.join(\n",
        "        results_folder, dataset_name, estimator_name\n",
        "    )\n",
        "    print(f\"Saving {estimation_result_folder}.\")\n",
        "    helper.save_in_and_out_results(\n",
        "        estimation_result_folder,\n",
        "        y0_in_all,\n",
        "        y1_in_all,\n",
        "        ate_in_all,\n",
        "        y0_out_all,\n",
        "        y1_out_all,\n",
        "        ate_out_all,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = \"DML\" #@param estimator_set = [\"OLS1\", \"OLS2\", \"NN1\", \"NN2\", \"RF1\", \"RF2\", \"Dragonnet\", \"TARNet\", \"DML\"]"
      ],
      "metadata": {
        "id": "B5YUTY8kNimq"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimate(estimator)"
      ],
      "metadata": {
        "id": "utH9AdZeWyx6",
        "outputId": "7fda1eba-b051-4dd9-97d1-62a45bc5c3d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numer of realizations: 1\n",
            "--------------------- Estimation of realization 0 via DML----------------------\n",
            "Saving ./estimation_results/TWINS/DML.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmAydp3Vk-1o"
      },
      "source": [
        "## Evalutation\n",
        " [6]\n",
        "![image](https://github.com/VectorInstitute/Causal_Inference_Laboratory/assets/47928320/e709567f-f280-4ac6-9572-5edeeef848e4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jaTQoY0Ik-1o"
      },
      "outputs": [],
      "source": [
        "def print_table(header, data):\n",
        "    table_width = 79\n",
        "    header_line = f'| {header.center(table_width - 4)} |'\n",
        "    separator_line = f'+{\"-\" * (table_width - 2)}+'\n",
        "\n",
        "    print(separator_line)\n",
        "    print(header_line)\n",
        "    print(separator_line)\n",
        "\n",
        "    for row in data:\n",
        "        row_line = f'| {row[0].ljust(30)} | {row[1].ljust(15)} | {str(row[2]).ljust(24)} |'\n",
        "        print(row_line)\n",
        "\n",
        "    print(separator_line)\n",
        "\n",
        "def evaluate(estimator_name, metrics_set, print_out=True):\n",
        "    if print_out:\n",
        "      print(f'{\" Evaluation \":-^79}')\n",
        "    results_in = {}\n",
        "    results_out = {}\n",
        "    if dataset_name == \"Jobs\":\n",
        "        x_all, t_all, yf_all = helper.load_Jobs_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        x_test_all, t_test_all, yf_test_all = helper.load_Jobs_out_of_sample(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        ate_in_gt, ate_out_gt = helper.load_Jobs_ground_truth(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        mu0_in, mu1_in, mu0_out, mu1_out = None, None, None, None\n",
        "    elif dataset_name == \"TWINS\":\n",
        "        x_all, t_all, yf_all = helper.load_TWINS_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        x_test_all, t_test_all, yf_test_all = helper.load_TWINS_out_of_sample(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        mu0_in, mu1_in, mu0_out, mu1_out = helper.load_TWINS_ground_truth(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        ate_in_gt = np.mean(mu1_in - mu0_in)\n",
        "        ate_out_gt = np.mean(mu1_out - mu0_out)\n",
        "    elif dataset_name == \"IHDP-100\":\n",
        "        x_all, t_all, yf_all = helper.load_IHDP_observational(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        x_test_all, t_test_all, yf_test_all = helper.load_IHDP_out_of_sample(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        mu0_in, mu1_in, mu0_out, mu1_out = helper.load_IHDP_ground_truth(\n",
        "            datasets_folder, dataset_name, details=False\n",
        "        )\n",
        "        ate_in_gt = np.mean(mu1_in - mu0_in)\n",
        "        ate_out_gt = np.mean(mu1_out - mu0_out)\n",
        "        yf_all = helper.scale_y(yf_all)\n",
        "        yf_test_all = helper.scale_y(yf_test_all)\n",
        "\n",
        "\n",
        "    # process in sample data\n",
        "    data_size_in = x_all.shape[0]\n",
        "    num_realizations_in = 1\n",
        "    if len(x_all.shape) == 3:\n",
        "        num_realizations_in = x_all.shape[2]\n",
        "        new_x_all = np.zeros((data_size_in * num_realizations_in, x_all.shape[1]))\n",
        "        for i in range(num_realizations_in):\n",
        "            new_x_all[i * data_size_in : (i + 1) * data_size_in, :] = x_all[:, :, i]\n",
        "        x_all = new_x_all\n",
        "\n",
        "\n",
        "    # squeeze all eval data\n",
        "    x_all = np.reshape(x_all, (data_size_in*num_realizations_in, x_all.shape[1]))\n",
        "    t_all = np.reshape(t_all, (data_size_in * num_realizations_in), order='F')\n",
        "    yf_all = np.reshape(yf_all, (data_size_in * num_realizations_in), order='F')\n",
        "\n",
        "    indices_all = np.arange(x_all.shape[0])\n",
        "\n",
        "    x_train, x_eval_orig, t_train, t_eval_orig, yf_train, yf_eval_orig, indices_train, indices_eval = train_test_split(\n",
        "        x_all, t_all, yf_all, indices_all, test_size=0.2, random_state=seed\n",
        "        )\n",
        "\n",
        "    # process out of sample data\n",
        "    data_size_out = x_test_all.shape[0]\n",
        "    num_realizations_out = 1\n",
        "    if len(x_test_all.shape) == 3:\n",
        "        num_realizations_out = x_test_all.shape[2]\n",
        "        new_x_test_all = np.zeros((data_size_out * num_realizations_out, x_test_all.shape[1]))\n",
        "        for i in range(num_realizations_out):\n",
        "            new_x_test_all[i * data_size_out : (i + 1) * data_size_out, :] = x_test_all[:, :, i]\n",
        "        x_test_all = new_x_test_all\n",
        "\n",
        "    # squeeze all test data\n",
        "    x_test_all = np.reshape(x_test_all, (data_size_out*num_realizations_out, x_test_all.shape[1]))\n",
        "    t_test_all = np.reshape(t_test_all, (data_size_out * num_realizations_out), order='F')\n",
        "    yf_test_all = np.reshape(yf_test_all, (data_size_out * num_realizations_out), order='F')\n",
        "\n",
        "    #Computing relevant evaluation metric for ensemble for in sample data\n",
        "    nuisance_stats_dir= results_folder + '//..//models//' + dataset_name + '//'\n",
        "    # Nuisance Models\n",
        "    prop_prob_orig, prop_score_orig = get_nuisance_propensity_pred(x_eval_orig, t_eval_orig, save_dir=nuisance_stats_dir)\n",
        "    outcome_s_pred = get_nuisance_outome_s_pred(x_eval_orig, t_eval_orig, save_dir=nuisance_stats_dir)\n",
        "    outcome_t_pred = get_nuisance_outcome_t_pred(x_eval_orig, t_eval_orig, save_dir=nuisance_stats_dir)\n",
        "    outcome_r_pred = get_nuisance_outcome_r_pred(x_eval_orig, save_dir=nuisance_stats_dir)\n",
        "\n",
        "    outcome_s_pred_orig = np.array(outcome_s_pred)\n",
        "    outcome_t_pred_orig = np.array(outcome_t_pred)\n",
        "    outcome_r_pred_orig = np.array(outcome_r_pred)\n",
        "\n",
        "    #Computing relevant evaluation metric for ensemble for out of sample data\n",
        "    # Nuisance Models\n",
        "    prop_prob_test, prop_score_test = get_nuisance_propensity_pred(x_test_all, t_test_all, save_dir=nuisance_stats_dir)\n",
        "    outcome_s_pred = get_nuisance_outome_s_pred(x_test_all, t_test_all, save_dir=nuisance_stats_dir)\n",
        "    outcome_t_pred = get_nuisance_outcome_t_pred(x_test_all, t_test_all, save_dir=nuisance_stats_dir)\n",
        "    outcome_r_pred = get_nuisance_outcome_r_pred(x_test_all, save_dir=nuisance_stats_dir)\n",
        "\n",
        "    outcome_s_pred_test = np.array(outcome_s_pred)\n",
        "    outcome_t_pred_test = np.array(outcome_t_pred)\n",
        "    outcome_r_pred_test = np.array(outcome_r_pred)\n",
        "\n",
        "    estimation_result_folder = os.path.join(\n",
        "        results_folder, dataset_name, estimator_name\n",
        "    )\n",
        "    (\n",
        "        y0_in,\n",
        "        y1_in,\n",
        "        ate_in,\n",
        "        y0_out,\n",
        "        y1_out,\n",
        "        ate_out,\n",
        "    ) = helper.load_in_and_out_results(estimation_result_folder)\n",
        "\n",
        "    if dataset_name == \"TWINS\":\n",
        "        y0_in = y0_in.reshape((-1, 1))\n",
        "        y1_in = y1_in.reshape((-1, 1))\n",
        "        y0_out = y0_out.reshape((-1, 1))\n",
        "        y1_out = y1_out.reshape((-1, 1))\n",
        "        ate_in = ate_in.reshape((-1, 1))\n",
        "        ate_out = ate_out.reshape((-1, 1))\n",
        "\n",
        "    if estimator_name == \"DML\":\n",
        "        # create dummy y0_in and y1_in and y0_out and y1_out\n",
        "        y0_in = np.zeros((t_all.shape[0], 1))\n",
        "        y1_in = np.zeros((t_all.shape[0], 1))\n",
        "        y0_out = np.zeros((t_test_all.shape[0], 1))\n",
        "        y1_out = np.zeros((t_test_all.shape[0], 1))\n",
        "\n",
        "    results_in[estimator_name] = {}\n",
        "    results_out[estimator_name] = {}\n",
        "\n",
        "    # process in sample data\n",
        "    ite_estimate_in = y1_in.reshape((-1, 1), order='F') - y0_in.reshape((-1, 1), order='F')\n",
        "    ite_estimate_eval = ite_estimate_in[indices_eval]\n",
        "\n",
        "    # get indices of non nan values\n",
        "    non_nan = ~np.isnan(ite_estimate_eval)\n",
        "    non_nan_inds = np.where(non_nan)[0]\n",
        "    ite_estimate_eval = ite_estimate_eval[non_nan_inds]\n",
        "    x_eval = np.take(x_eval_orig, non_nan_inds, axis=0)\n",
        "    t_eval = t_eval_orig[non_nan.squeeze()]\n",
        "    yf_eval = yf_eval_orig[non_nan.squeeze()]\n",
        "\n",
        "    prop_score = prop_score_orig[non_nan.squeeze()]\n",
        "    outcome_s_pred = np.transpose(np.transpose(outcome_s_pred_orig)[non_nan.squeeze()])\n",
        "    outcome_t_pred = np.transpose(np.transpose(outcome_t_pred_orig)[non_nan.squeeze()])\n",
        "    outcome_r_pred = outcome_r_pred_orig[non_nan.squeeze()]\n",
        "    prop_prob = prop_prob_orig[non_nan.squeeze()]\n",
        "\n",
        "    # process out of sample data\n",
        "    ite_estimate_out = y1_out.reshape((-1, 1), order='F') - y0_out.reshape((-1, 1), order='F')\n",
        "    ite_estimate_eval_out = ite_estimate_out\n",
        "\n",
        "    # get indices of non nan values\n",
        "    non_nan = ~np.isnan(ite_estimate_eval_out)\n",
        "    non_nan_inds = np.where(non_nan)[0]\n",
        "    ite_estimate_eval_out = ite_estimate_eval_out[non_nan_inds]\n",
        "    x_eval_out = np.take(x_test_all, non_nan_inds, axis=0)\n",
        "    t_eval_out = t_test_all[non_nan.squeeze()]\n",
        "    yf_eval_out = yf_test_all[non_nan.squeeze()]\n",
        "\n",
        "    prop_score_out = prop_score_test[non_nan.squeeze()]\n",
        "    outcome_s_pred_out = np.transpose(np.transpose(outcome_s_pred_test)[non_nan.squeeze()])\n",
        "    outcome_t_pred_out = np.transpose(np.transpose(outcome_t_pred_test)[non_nan.squeeze()])\n",
        "    outcome_r_pred_out = outcome_r_pred_test[non_nan.squeeze()]\n",
        "    prop_prob_out = prop_prob_test[non_nan.squeeze()]\n",
        "\n",
        "    for metric in metrics_set:\n",
        "        metric_in = None\n",
        "        if metric in [\"MAE\", \"PEHE\"] and estimator_name == \"DML\":\n",
        "            metric_in = np.abs(ate_in_gt - ate_in)\n",
        "            metric_out = np.abs(ate_out_gt - ate_out)\n",
        "        elif metric in [\"MAE\", \"PEHE\"]:\n",
        "            metric_in = metrics.calculate_metrics(\n",
        "                y0_in, y1_in, ate_in, mu0_in, mu1_in, ate_in_gt, metric=metric\n",
        "            )\n",
        "            metric_out = metrics.calculate_metrics(\n",
        "                y0_out, y1_out, ate_out, mu0_out, mu1_out, ate_out_gt, metric=metric\n",
        "            )\n",
        "        elif metric == \"value_score\":\n",
        "            metric_in = metrics.calculate_value_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, dataset_name=dataset_name, prop_score=prop_score\n",
        "            )\n",
        "            metric_out = metrics.calculate_value_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, dataset_name=dataset_name, prop_score=prop_score_out\n",
        "            )\n",
        "        elif metric == \"value_dr_score\":\n",
        "            metric_in = metrics.calculate_value_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, dataset_name=dataset_name, prop_score=prop_score\n",
        "            )\n",
        "            metric_out = metrics.calculate_value_dr_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_t_pred_out, dataset_name=dataset_name, prop_score=prop_score_out\n",
        "            )\n",
        "        elif metric == \"value_dr_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_value_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, dataset_name=dataset_name, prop_score=prop_score, min_propensity=0.1\n",
        "            )\n",
        "            metric_out = metrics.calculate_value_dr_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_t_pred_out, dataset_name=dataset_name, prop_score=prop_score_out, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"tau_match_score\":\n",
        "            metric_in = metrics.calculate_tau_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval\n",
        "            )\n",
        "            metric_out = metrics.calculate_tau_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out\n",
        "            )\n",
        "        elif metric == \"tau_iptw_score\":\n",
        "            metric_in = metrics.calculate_tau_iptw_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, prop_score=prop_score\n",
        "            )\n",
        "            metric_out = metrics.calculate_tau_iptw_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, prop_score=prop_score_out\n",
        "            )\n",
        "        elif metric == \"tau_iptw_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_tau_iptw_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, prop_score=prop_score, min_propensity=0.1\n",
        "            )\n",
        "            metric_out = metrics.calculate_tau_iptw_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, prop_score=prop_score_out, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"tau_dr_score\":\n",
        "            metric_in = metrics.calculate_tau_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_score=prop_score\n",
        "            )\n",
        "            metric_out = metrics.calculate_tau_dr_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_t_pred_out, prop_score=prop_score_out\n",
        "            )\n",
        "        elif metric == \"tau_dr_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_tau_dr_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_score=prop_score, min_propensity=0.1\n",
        "            )\n",
        "            metric_out = metrics.calculate_tau_dr_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_t_pred_out, prop_score=prop_score_out, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"tau_s_score\":\n",
        "            metric_in = metrics.calculate_tau_s_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_s_pred\n",
        "            )\n",
        "            metric_out = metrics.calculate_tau_s_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_s_pred_out\n",
        "            )\n",
        "        elif metric == \"tau_t_score\":\n",
        "            metric_in = metrics.calculate_tau_t_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred\n",
        "            )\n",
        "            metric_out = metrics.calculate_tau_t_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_t_pred_out\n",
        "            )\n",
        "        elif metric == \"influence_score\":\n",
        "            metric_in = metrics.calculate_influence_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_prob=prop_prob\n",
        "            )\n",
        "            metric_out = metrics.calculate_influence_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_t_pred_out, prop_prob=prop_prob_out\n",
        "            )\n",
        "        elif metric == \"influence_clip_prop_score\":\n",
        "            metric_in = metrics.calculate_influence_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_t_pred, prop_prob=prop_prob, min_propensity=0.1\n",
        "            )\n",
        "            metric_out = metrics.calculate_influence_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_t_pred_out, prop_prob=prop_prob_out, min_propensity=0.1\n",
        "            )\n",
        "        elif metric == \"r_score\":\n",
        "            metric_in = metrics.calculate_r_risk(\n",
        "                ite_estimate_eval, x_eval, t_eval, yf_eval, outcome_pred=outcome_r_pred, treatment_prob=prop_prob[:, 1]\n",
        "            )\n",
        "            metric_out = metrics.calculate_r_risk(\n",
        "                ite_estimate_eval_out, x_eval_out, t_eval_out, yf_eval_out, outcome_pred=outcome_r_pred_out, treatment_prob=prop_prob_out[:, 1]\n",
        "            )\n",
        "        elif metric == \"abs_diff_ate_t\":\n",
        "            metric_in = metrics.calculate_abs_diff_ate(\n",
        "                ate_in, outcome_pred=outcome_t_pred\n",
        "            )\n",
        "            metric_out = metrics.calculate_abs_diff_ate(\n",
        "                ate_out, outcome_pred=outcome_t_pred_out\n",
        "            )\n",
        "        elif metric == \"abs_diff_ate_s\":\n",
        "            metric_in = metrics.calculate_abs_diff_ate(\n",
        "                ate_in, outcome_pred=outcome_s_pred\n",
        "            )\n",
        "            metric_out = metrics.calculate_abs_diff_ate(\n",
        "                ate_out, outcome_pred=outcome_s_pred_out\n",
        "            )\n",
        "\n",
        "        if metric_in is None:\n",
        "            results_in[estimator_name][metric] = {\"mean\": None}\n",
        "        else:\n",
        "            results_in[estimator_name][metric] = {\n",
        "                \"mean\": np.mean(metric_in, where=(metric_in != 0)),\n",
        "            }\n",
        "        if metric_out is None:\n",
        "            results_out[estimator_name][metric] = {\"mean\": None}\n",
        "        else:\n",
        "            results_out[estimator_name][metric] = {\n",
        "                \"mean\": np.mean(metric_out, where=(metric_out != 0)),\n",
        "            }\n",
        "\n",
        "    if print_out:\n",
        "      # In-sample results\n",
        "      header_in = f' In-sample results '\n",
        "      data_in = [[metric, estimator_name, results_in[estimator_name][metric][\"mean\"]] for metric in metrics_set]\n",
        "\n",
        "      # Out-of-sample results\n",
        "      header_out = f' Out-of-sample results '\n",
        "      data_out = [[metric, estimator_name, results_out[estimator_name][metric][\"mean\"]] for metric in metrics_set]\n",
        "\n",
        "      print_table(header_in, data_in)\n",
        "      print_table(header_out, data_out)\n",
        "\n",
        "    return results_in, results_out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select which estimator you want to evaluate below."
      ],
      "metadata": {
        "id": "2Uzy4JDpM0AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = \"OLS2\" #@param estimator_set = [\"OLS1\", \"OLS2\", \"NN1\", \"NN2\", \"RF1\", \"RF2\", \"Dragonnet\", \"TARNet\", \"DML\"]"
      ],
      "metadata": {
        "id": "FK41I9biBEa0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the metric (or \"ALL\") from below to retreive the value. Ensure you've selected an estimator at the previous step."
      ],
      "metadata": {
        "id": "sCArxGr7WoT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = \"ALL\" #@param [ \"MAE\", \"PEHE\", \"value_score\", \"value_dr_score\", \"value_dr_clip_prop_score\", \"tau_t_score\", \"tau_s_score\", \"tau_match_score\", \"tau_iptw_score\", \"tau_iptw_clip_prop_score\", \"tau_dr_score\", \"tau_dr_clip_prop_score\", \"influence_score\", \"influence_clip_prop_score\", \"r_score\", \"abs_diff_ate_t\", \"abs_diff_ate_s\", \"ALL\", \"DML_ALL\"]\n",
        "if metric == \"ALL\":\n",
        "  metric_set = [ \"MAE\", \"PEHE\", \"value_score\", \"value_dr_score\", \"value_dr_clip_prop_score\", \"tau_t_score\", \"tau_s_score\", \"tau_match_score\", \"tau_iptw_score\", \"tau_iptw_clip_prop_score\", \"tau_dr_score\", \"tau_dr_clip_prop_score\", \"influence_score\", \"influence_clip_prop_score\", \"r_score\", \"abs_diff_ate_t\", \"abs_diff_ate_s\"]\n",
        "elif \"DML_ALL\":\n",
        "  metric_set = [\"MAE\", \"abs_diff_ate_t\", \"abs_diff_ate_s\"]\n",
        "else:\n",
        "  metric_set = [metric]\n",
        "\n",
        "evaluate(estimator, metric_set)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "GkyNYLSwdVeW",
        "outputId": "c9b68d67-30c7-4788-a661-a1c0e73203d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------- Evaluation ----------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-395fdec4ea1b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmetric_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-492ecc350367>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(estimator_name, metrics_set, print_out)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Nuisance Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprop_prob_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_score_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nuisance_propensity_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_eval_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnuisance_stats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0moutcome_s_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nuisance_outome_s_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_eval_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnuisance_stats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0moutcome_t_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nuisance_outcome_t_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_eval_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnuisance_stats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0moutcome_r_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nuisance_outcome_r_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnuisance_stats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils/evaluation.py\u001b[0m in \u001b[0;36mget_nuisance_outome_s_pred\u001b[0;34m(w, t, save_dir)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mout_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'mu_s'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mmu_0\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mout_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmu_1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mout_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;31m# retrive original params that possibly can be used in both training and prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;31m# and then overwrite them (considering aliases) with params that were passed directly in prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0mpredict_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m         for alias in _ConfigAliases.get_by_alias(\n\u001b[1;32m    901\u001b[0m             \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36m_process_params\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0malias\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_ConfigAliases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare All Estimators\n",
        "\n",
        "Run the following code to run the evaluation code on all the estimators and output a table.\n",
        "\n"
      ],
      "metadata": {
        "id": "M16jyHCulFdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator_set = [\"OLS1\", \"OLS2\", \"NN1\", \"NN2\", \"RF1\", \"RF2\", \"Dragonnet\", \"TARNet\", \"DML\"]\n",
        "overall_res_in = {}\n",
        "overall_res_out = {}\n",
        "for estimator in estimator_set:\n",
        "  res_in, res_out = evaluate(estimator, metric_set, False)\n",
        "  for metric in metric_set:\n",
        "    res_in[estimator][metric] = str(res_in[estimator][metric][\"mean\"])\n",
        "    res_out[estimator][metric] = str(res_out[estimator][metric][\"mean\"])\n",
        "  overall_res_in.update(res_in)\n",
        "  overall_res_out.update(res_out)"
      ],
      "metadata": {
        "id": "Yr9LLRoCleTh"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "def print_dict_as_table(dictionary):\n",
        "    headers = ['Metric'] + list(dictionary.keys())\n",
        "    rows = []\n",
        "    inner_keys = list(dictionary[headers[1]].keys())\n",
        "\n",
        "    for key in inner_keys:\n",
        "        row = [key] + [dictionary[header][key] for header in headers[1:]]\n",
        "        rows.append(row)\n",
        "\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "print(\"In-Sample\")\n",
        "print_dict_as_table(overall_res_in)\n",
        "print(\"Out-of-Sample\")\n",
        "print_dict_as_table(overall_res_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek8y9cAysmfU",
        "outputId": "967bb161-05f0-42a9-a2ca-16e03db08415"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-Sample\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| Metric                    |       OLS1 |       OLS2 |        NN1 |        NN2 |        RF1 |        RF2 |   Dragonnet |     TARNet |        DML |\n",
            "+===========================+============+============+============+============+============+============+=============+============+============+\n",
            "| MAE                       |   0.74638  |   0.135197 |   0.294896 |   0.438299 |   0.471533 |   0.118151 |    0.144033 |   0.173731 |   4.33996  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| PEHE                      |   5.60018  |   2.23514  |   1.31518  |   1.47209  |   3.02372  |   1.71315  |    0.619652 |   0.696516 |   4.33996  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| value_score               |  -0.111324 |  -0.144588 |  -0.139844 |  -0.144704 |  -0.144833 |  -0.145084 |   -0.149939 |  -0.147905 |  -0.24764  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| value_dr_score            | -10.2587   |  -9.16688  |  -9.35234  |  -9.23743  |  -9.27142  |  -9.26175  |   -9.66815  |  -9.62799  | nan        |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| value_dr_clip_prop_score  |   1.90085  |   1.76019  |   1.80141  |   1.80267  |   2.06275  |   1.9415   |    1.96364  |   1.97629  | nan        |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_t_score               |   8.18336  |  76.7291   |  83.0369   |  93.1013   |  41.1303   |  74.0035   |   10.5664   |  11.2841   |  27.022    |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_s_score               |   1.2931   |  80.5143   |  86.1132   |  95.8401   |  42.1371   |  77.3563   |    6.47703  |   7.54909  |  17.2184   |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_match_score           |  14.5682   | 101.198    | 105.765    | 113.715    |  65.3732   |  98.1155   |   20.0396   |  21.7775   |   0.120277 |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_iptw_score            |  17.5666   | 104.807    | 109.319    | 117.05     |  69.3131   | 101.746    |   23.3543   |  25.0755   |   0.177304 |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_iptw_clip_prop_score  |  14.9921   |  90.6898   | 115.171    | 130.467    |  55.0456   |  85.0206   |   19.7313   |  21.2009   |   0.366143 |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_dr_score              | 177.683    | 248.237    | 254.132    | 268.128    | 202.47     | 244.63     |  182.745    | 183.237    | 243.276    |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_dr_clip_prop_score    | 312.73     | 386.318    | 409.008    | 421.887    | 353.555    | 387.047    |  319.725    | 322.821    | 281.81     |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| influence_score           |  37.0549   | 125.024    | 129.403    | 137.162    |  89.3482   | 122.867    |   43.3339   |  45.0931   |  21.3982   |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| influence_clip_prop_score |   6.89482  |  79.1124   | 100.977    | 114.699    |  45.7787   |  76.4862   |   12.3637   |  13.8819   |  -7.51227  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| r_score                   | 145.759    | 146.324    | 146.434    | 146.555    | 146.02     | 146.375    |  145.114    | 145.307    | 145.5      |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| abs_diff_ate_t            |   0.551191 |   0.912439 |   0.950288 |   0.887444 |   1.06659  |   0.952733 |  nan        | nan        |   4.19629  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| abs_diff_ate_s            |   0.272737 |   0.823397 |   0.862693 |   0.748987 |   1.05167  |   0.882397 |  nan        | nan        |   3.83345  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "Out-of-Sample\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| Metric                    |       OLS1 |       OLS2 |        NN1 |        NN2 |        RF1 |        RF2 |   Dragonnet |     TARNet |        DML |\n",
            "+===========================+============+============+============+============+============+============+=============+============+============+\n",
            "| MAE                       |   0.783886 |   0.269446 |   0.328718 |   0.389007 |   0.664562 |   0.33251  |    0.155925 |   0.175049 |   4.06657  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| PEHE                      |   5.73     |   2.40905  |   1.5826   |   1.65692  |   3.39202  |   2.69823  |    0.649325 |   0.715305 |   4.06657  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| value_score               |  -0.135216 |  -0.177144 |  -0.17214  |  -0.179019 |  -0.172607 |  -0.173965 |   -0.181384 |  -0.179117 |  -0.272853 |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| value_dr_score            | -10.2225   |  -9.17405  |  -9.37594  |  -9.21641  |  -9.35617  |  -9.31569  |   -9.63448  |  -9.64131  | nan        |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| value_dr_clip_prop_score  |   1.21974  |   0.588331 |   0.653757 |   0.626763 |   1.01022  |   0.790071 |    1.22996  |   0.931451 | nan        |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_t_score               |   8.28822  |  73.2487   |  81.5509   |  90.3096   |  35.3401   |  59.6747   |   10.7475   |  11.6937   |  26.8372   |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_s_score               |   1.26071  |  76.3398   |  83.9115   |  92.3739   |  34.9352   |  60.9867   |    6.3886   |   7.4926   |  17.1264   |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_match_score           |  14.0367   |  94.6802   | 101.464    | 108.157    |  56.0274   |  79.5988   |   19.2715   |  20.7963   |   0.171892 |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_iptw_score            |  17.6103   |  98.8597   | 105.476    | 111.937    |  60.6013   |  83.7955   |   23.2341   |  24.8131   |   0.240047 |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_iptw_clip_prop_score  |  14.7006   |  86.5173   | 112.967    | 132.738    |  47.6393   |  81.6489   |   18.9072   |  20.7594   |   0.489793 |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_dr_score              | 175.807    | 240.331    | 251.57     | 263.947    | 193.992    | 225.442    |  179.88     | 180.05     | 241.04     |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| tau_dr_clip_prop_score    | 303.989    | 359.165    | 389.495    | 406.975    | 331.207    | 362.145    |  302.53     | 301.884    | 277.691    |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| influence_score           |  37.2864   | 120.471    | 127.488    | 134.27     |  81.1916   | 105.04     |   43.5263   |  45.1154   |  21.8235   |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| influence_clip_prop_score |   9.1946   |  81.2057   | 108.51     | 128.355    |  42.7915   |  76.4338   |   13.1717   |  15.2103   |  -4.3134   |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| r_score                   | 144.688    | 144.854    | 145.171    | 145.322    | 144.737    | 144.949    |  143.885    | 144.046    | 144.571    |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| abs_diff_ate_t            |   0.518556 |   0.863229 |   0.913979 |   0.93403  |   0.984246 |   0.858735 |  nan        | nan        |   4.15921  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n",
            "| abs_diff_ate_s            |   0.268627 |   0.819237 |   0.840875 |   0.826426 |   0.977398 |   0.820803 |  nan        | nan        |   3.82279  |\n",
            "+---------------------------+------------+------------+------------+------------+------------+------------+-------------+------------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "[1] J. L. Hill, “Bayesian nonparametric modeling for causal inference,” Journal\n",
        "of Computational  and Graphical Statistics, vol. 20, no. 1, pp. 217–240, 2011.\n",
        "[Online]. Available: https://doi.org/10.1198/jcgs.2010.08162\n",
        "\n",
        "[2] R. J. LaLonde, “Evaluating the econometric evaluations of training programs\n",
        "with experimental data,” The American Economic Review, vol. 76, no. 4, pp.\n",
        "604–620, 1986. [Online]. Available: http://www.jstor.org/stable/1806062\n",
        "\n",
        "[3] U. Shalit, F. D. Johansson, and D. Sontag, “Estimating individual treatment\n",
        "effect: generalization bounds and algorithms,” in Proceedings of the 34th\n",
        "International Conference on Machine Learning, ser. Proceedings of Machine\n",
        "Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70. PMLR, 06–11 Aug 2017\n",
        ", pp. 3076–3085. [Online].\n",
        "Available: https://proceedings.mlr.press/v70/shalit17a.html\n",
        "\n",
        "[4] Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17), 6449–6459, 2017.\n",
        "\n",
        "[5] D. Almond, K. Y. Chay, and D. S. Lee. The costs of low birth weight.The Quarterly Journal of Economics,120(3):1031–1083, 2005.\n",
        "\n",
        "[6] D. Mahajan, I. Mitliagkas, B. Neal, and V. Syrgkanis, ‘Empirical Analysis of Model Selection for Heterogenous Causal Effect Estimation’, arXiv [cs.LG]. 2022.\n"
      ],
      "metadata": {
        "id": "6LqqReqIlQZg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a0b5a31022a61b32d9bd73ec997b0a079a20f27350e8c7766473b50790ea745"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}